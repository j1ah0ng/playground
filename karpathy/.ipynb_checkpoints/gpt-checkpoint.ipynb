{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efc5902-06d1-4151-bd77-fd5ff17bdbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-17 11:25:58--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.56MB/s    in 0.4s    \n",
      "\n",
      "2024-08-17 11:25:59 (2.56 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b37623-63d0-4f86-a135-cb95534f1527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length = 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(f\"length = {len(text)}\")\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb992240-6790-461e-a67e-c6c3d05eeed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd341b5c-0abf-41e0-ab5a-b61719488b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# character-level language model\n",
    "# we tokenize based on characters\n",
    "\n",
    "# encode chars -> ints\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "420218ee-0026-4fd4-81c2-7bab839daecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode entire dataset\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "device = torch.device(\"cpu\")\n",
    "data = data.to(device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "56343135-cb02-427c-bf4b-7b855e737280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5b5c057b-d587-474b-9927-e13761d78794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we train on chunks at a time. chunks have a maximum length\n",
    "block_size = 8\n",
    "train_data[:block_size + 1]\n",
    "# append one (get nine characters) because we are making eight predictions, \n",
    "# including on the unseen (ninth) char, excluding the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "08a69228-cd5c-41ec-a0cf-d06fc3727c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = tensor([18]) target = 47\n",
      "input = tensor([18, 47]) target = 56\n",
      "input = tensor([18, 47, 56]) target = 57\n",
      "input = tensor([18, 47, 56, 57]) target = 58\n",
      "input = tensor([18, 47, 56, 57, 58]) target = 1\n",
      "input = tensor([18, 47, 56, 57, 58,  1]) target = 15\n",
      "input = tensor([18, 47, 56, 57, 58,  1, 15]) target = 47\n",
      "input = tensor([18, 47, 56, 57, 58,  1, 15, 47]) target = 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'input = {context} target = {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9cf351da-57b2-4a0f-9811-107fd8d53cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "torch.Size([4, 8]) tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # minibatch\n",
    "block_size = 8 # context\n",
    "n_embd = 32 # hidden layer dimension\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,), device=device)\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, xb)\n",
    "print(yb.shape, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "522b0859-10bc-4b24-a0a2-25953eabb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "45e9ba6b-7876-497b-86a7-896a9b0c8146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n",
      "4.627182960510254\n",
      "n_params = 4481\n"
     ]
    }
   ],
   "source": [
    "# simplest possible neural network\n",
    "# bigram!\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # gives us token embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "\n",
    "        # embedding corresponding to the position of a token\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "        \n",
    "        # \"language model head\"\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (batchsize, ntoks) tensors\n",
    "        # for each token, we are predicting the probabilities of the next tokens\n",
    "        tok_emb = self.token_embedding_table(idx) # (batchsize, ntoks, embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (bs, embd)\n",
    "        \n",
    "        x = tok_emb + pos_emb \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # unpack the token predictions\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to the last #block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # look at the last time step to predict what comes next\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = Bigram()\n",
    "logits, loss = m(xb, yb)\n",
    "print(xb.shape)\n",
    "print(yb.shape)\n",
    "# print(out.shape)\n",
    "print(loss.item())\n",
    "print(f\"n_params = {sum(p.nelement() for p in m.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c3d017d2-1a17-403d-b68d-8b49015cd049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.174387269895637)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expected loss should be random -- prob is uniform\n",
    "import numpy as np\n",
    "uniform_prob = 1 / vocab_size\n",
    "-np.log(uniform_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "60400d9f-d9d3-4b5a-a989-47168b7b3e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SmVddomOVWydk3'BrlK\n",
      "QduWGGHPmiu&UXBlIHTZ'yfsDuEtqWPUlOZt&-lV&qBohwN.l;N3z:miimwvg,gAo3EPN3hOw$!VyTuE\n"
     ]
    }
   ],
   "source": [
    "# the batch. holds a zero. kicks off the generation here\n",
    "idx = torch.zeros((1, 1), dtype = torch.long, device=device)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bda31630-1a00-4ed4-9aa7-02d4a9afeebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b4b36948-51b1-46ab-8b41-9217bd3ab18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/  10000: loss=2.5254, Tavg=0.00579\n",
      "   1000/  10000: loss=2.4901, Tavg=0.00075\n",
      "   2000/  10000: loss=2.5363, Tavg=0.00074\n",
      "   3000/  10000: loss=2.4859, Tavg=0.00074\n",
      "   4000/  10000: loss=2.3667, Tavg=0.00074\n",
      "   5000/  10000: loss=2.5293, Tavg=0.00073\n",
      "   6000/  10000: loss=2.5339, Tavg=0.00073\n",
      "   7000/  10000: loss=2.4117, Tavg=0.00073\n",
      "   8000/  10000: loss=2.4818, Tavg=0.00073\n",
      "   9000/  10000: loss=2.4348, Tavg=0.00075\n",
      "2.3926258087158203\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # usually, 3e-4 but for stupid networks you can learn faster\n",
    "batch_size = 32\n",
    "DTbar = 0\n",
    "\n",
    "for steps in range(10_000):\n",
    "    Ta = time_ns()\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    DT = time_ns() - Ta\n",
    "    DTbar = (DTbar*steps + DT)/(steps+1)\n",
    "\n",
    "    if steps % 1_000 == 0:\n",
    "        print(f\"{steps:7d}/{10_000:7d}: loss={loss.item():1.4f}, Tavg={DTbar/1e9:.5f}\")\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6f93d-7058-436f-ade5-bd6ae9439ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.zeros((1, 1), dtype = torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f0810-129b-4c95-bdf7-40520f98a8da",
   "metadata": {},
   "source": [
    "# self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "348c05f6-12f9-4523-9098-c3d9446a22ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy ex\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch=4, time=8, channels=2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad578473-8aaa-4e4e-83a7-82163c20f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information should flow forwards only: e.g. token at position n can receive info from positions [0, n-1]\n",
    "# simplest way of doing this is to take an average of all the previous tokens\n",
    "xbow = torch.zeros((B, T, C)) # bag of words, averaged prev tokens\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1b045c1-fb44-4d05-8be1-69ea6d1ffa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7e86fc2-c1ae-4236-9c62-60dd0d781153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# this is inefficient. an just matmul\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7732d3c5-423a-4dec-b7fa-16f2161b0695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  7.],\n",
       "        [ 8., 11.],\n",
       "        [14., 16.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can just build a diagonal matrix\n",
    "at = torch.tril(torch.ones(3, 3)) \n",
    "at @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c38d6f6-be6d-4e39-ad0b-75124fd30d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 7.0000],\n",
       "        [4.0000, 5.5000],\n",
       "        [4.6667, 5.3333]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tril @ (T, B, ...) -> (T, B, ...)\n",
    "# now we can get averages\n",
    "(at / torch.sum(at, 1, keepdim=True)) @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb0c7d91-28d6-4d4f-b746-e2aca74e4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_ones = torch.ones(T, T)\n",
    "T_tril = torch.tril(T_ones)\n",
    "T_avgk = T_tril / torch.sum(T_tril, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c83160a1-468e-43a9-950f-078cb2c254ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3199, -0.0396, -0.0682,  0.0084,  0.0020,  0.0712, -0.1128,  0.2527,\n",
      "        -0.0436, -0.2593, -0.3015,  0.4954,  0.3420,  1.1401, -0.4462,  1.0870,\n",
      "        -0.4071, -0.1641])\n",
      "tensor([-0.3199, -0.0396, -0.0682,  0.0084,  0.0020,  0.0712, -0.1128,  0.2527,\n",
      "        -0.0436, -0.2593, -0.3015,  0.4954,  0.3420,  1.1401, -0.4462,  1.0870,\n",
      "        -0.4071, -0.1641])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "xbow2 = T_avgk @ x\n",
    "print(xbow[xbow != xbow2])\n",
    "print(xbow2[xbow != xbow2])\n",
    "print(torch.allclose(xbow, xbow2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0cfb0a3-9370-4219-9f5e-29e8d3dc328b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can use softmax\n",
    "# it basically tells us how significantly tokens interact\n",
    "# --> \"affinities\" for certain tokens in the past\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular ones\n",
    "wei = torch.zeros((T, T)) # all zeros\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # fill the upper triangular portion with -inf\n",
    "wei = F.softmax(wei, dim=-1) # softmax along every row -> .exp().div(sum(dim=1))\n",
    "torch.allclose(wei @ x, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d186071-8f8a-4687-af4e-25ee002ff0ec",
   "metadata": {},
   "source": [
    "lower triangular multiplication gives us how much affinity a token in the ith row has for the previous rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70848bc3-aec5-4906-a6bf-0d2a06dcfcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy ex\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch=4, time=8, channels=2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular ones\n",
    "wei = torch.zeros((T, T)) # all zeros\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # fill the upper triangular portion with -inf\n",
    "wei = F.softmax(wei, dim=-1) # softmax along every row -> .exp().div(sum(dim=1))\n",
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a49c471-036a-453d-9a58-b3e3f5fb2275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ffe036e9-086c-4db3-ae13-cde022517deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f278c-36f2-449d-ab93-b860539ac59c",
   "metadata": {},
   "source": [
    "every token at every position gives two vectors : a query, and a key. the query vector is \"what am i looking for\", and the key is \"what do i contain\"\n",
    "\n",
    "affinities: dot product between keys and query -> dot product gives weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47ea82bf-b65c-46d0-9643-529cc18429fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy ex\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch=4, time=8, channels=3 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# key learns what the token contains\n",
    "# query is what the token wants\n",
    "\n",
    "# all queries dot all the keys\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T). \n",
    "# We now get a T x T matrix containing the weights\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular ones\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # fill the upper triangular portion with -inf\n",
    "wei = F.softmax(wei, dim=-1) # softmax along every row -> .exp().div(sum(dim=1))\n",
    "\n",
    "# x is \"private info\" to the token\n",
    "# v is the thing that is aggregated between nodes in a given window\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f4aef-9a69-49dc-a4e4-ff5122d6adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that wei has no notion of position\n",
    "# this is why we encode positions also\n",
    "\n",
    "# we also may not need the tril masking: here we use it for autoregressive text generation\n",
    "# for other use cases, you could unmask it\n",
    "\n",
    "# \"self-attention\" is looking within the context as Q, K, V come from x.\n",
    "# \"cross-attention\" is when a separate source of information is used\n",
    "\n",
    "# \"scaled\" attention normalizes by sqrt(head size)\n",
    "# to keep the variance of the weight matrix diffuse and disallow softmax from saturating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d14d1e88-01cb-425d-9ff5-4fe64ac5398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9957)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(B, T, head_size) @ torch.randn(B, T, head_size).mT * (head_size**-.5)).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5395edeb-1d5a-4740-b280-63070a1e5e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "29ab3461-4293-4746-a7c6-69cbd63609bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wei is now data dependent\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "079f072e-7d6b-46c1-a760-6950518f34fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59031dd9-f001-4950-a55b-c3100eeac7cb",
   "metadata": {},
   "source": [
    "# attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f732e031-e753-4ad4-a99f-91ebd19dc33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)        \n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # compute self-attention = QK^T / sqrt(head_size)\n",
    "        # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        wei = q @ k.mT * C**-0.5\n",
    "        # drop out the forward connections\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # LT (B, T, T)\n",
    "        # softmax to get multinomials\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55d56c01-3fd5-4c2f-9b49-59158d806475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e94cbd62-93ec-4621-8441-24835823fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),   \n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a9a49d34-8281-41dc-a205-ff6a0b44d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim, device=device)\n",
    "        self.beta = torch.zeros(dim, device=device)\n",
    "    def forward(self, x):\n",
    "        xmean = x.mean(1, keepdim=True) # \"layer\" mean: across each context window\n",
    "        xvar = x.var(1, keepdim=True)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # zscore\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "34b18a0b-3d4b-4328-bba1-a07272432d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        assert(head_size * n_head == n_embd)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        # residual connections by adding self \n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8ed5cdcc-76f1-433c-aad9-3a2f67b18522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8])\n",
      "4.241476535797119\n",
      "n_params = 54977\n"
     ]
    }
   ],
   "source": [
    "# simplest possible neural network\n",
    "# bigram!\n",
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # gives us token embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # embedding corresponding to the position of a token\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # language output\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (batchsize, ntoks) tensors\n",
    "        # for each token, we are predicting the probabilities of the next tokens\n",
    "        tok_emb = self.token_embedding_table(idx) # (batchsize, ntoks, embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (bs, embd)\n",
    "        \n",
    "        x = tok_emb + pos_emb \n",
    "\n",
    "        # invoke self-attention\n",
    "        x = self.blocks(x)\n",
    "        # layernorm before logits\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # unpack the token predictions\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to the last #block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # look at the last time step to predict what comes next\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel()\n",
    "logits, loss = m(xb, yb)\n",
    "print(xb.shape)\n",
    "print(yb.shape)\n",
    "# print(out.shape)\n",
    "print(loss.item())\n",
    "print(f\"n_params = {sum(p.nelement() for p in m.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c780aa58-a2c6-47bb-8320-15e93ba6626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/   5000: loss=4.4321, Tavg=0.03138\n",
      "   1000/   5000: loss=2.3600, Tavg=0.00938\n",
      "   2000/   5000: loss=2.2183, Tavg=0.00928\n",
      "   3000/   5000: loss=2.0285, Tavg=0.00981\n",
      "   4000/   5000: loss=2.0215, Tavg=0.01011\n",
      "   5000/   5000: loss=2.0209, Tavg=0.01029\n"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(9999)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # usually, 3e-4 but for stupid networks you can learn faster\n",
    "batch_size = 32\n",
    "max_s = 5_000\n",
    "for steps in range(max_s):\n",
    "\n",
    "    Ta = time_ns()\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    DT = time_ns() - Ta\n",
    "    DTbar = (DTbar*steps + DT)/(steps+1)\n",
    "\n",
    "    if steps % 1_000 == 0:\n",
    "        print(f\"{steps:7d}/{max_s:7d}: loss={loss.item():1.4f}, Tavg={DTbar/1e9:.5f}\")\n",
    "    \n",
    "print(f\"{max_s:7d}/{max_s:7d}: loss={loss.item():1.4f}, Tavg={DTbar/1e9:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9e288145-4fee-4b72-8303-56a6112ae927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(2.0014), 'val': tensor(2.0837)}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss(m, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6cfffbfd-863f-480f-9a10-66cb202712e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The many rimfent I frieds:\n",
      "Theow that in that mane! I cishvilds.\n",
      "\n",
      "3IVOR:\n",
      "But therere cally?\n",
      "Then likse heatight't puress the my?\n",
      "\n",
      "INCE:\n",
      "O ketort delile. Yoush wish selik'd live thy, mistrest:\n",
      "I bret faithor sefpelf if you druived is an than?\n",
      "Hatther armaus a a a fighter:\n",
      "Ather Lords lians we lang!\n",
      "\n",
      "\n",
      "PARYOME:\n",
      "Where your to ine lie, and and honirsint you where anctle's?\n",
      "\n",
      "CLIR\n",
      "HANUCICING TI:\n",
      "And is and maven me re,\n",
      "And nargiagh is the that tose sut. \n",
      "HARDWLANGER: VOucke you with thank thire now yevouranst he our lut het, morlave.\n",
      "\n",
      "PERLINGENENY EDW:\n",
      "Hinks!\n",
      "\n",
      "CHUERN:\n",
      "I the tle brice arce ent yeg\n",
      "four their majy hearroh he mosion I lid fitiens;\n",
      "Sralain.\n",
      "\n",
      "ORFLANUE:\n",
      "And the fulses,\n",
      "What dleatile ald to weef. I Yake and way yet, true, wifish hath Wereals\n",
      "plorous faind beationd law;\n",
      "Tt\n",
      "now Isensend the and carce that\n",
      "slange; but.\n",
      "\n",
      "StANDWAEINIU:\n",
      "Where stort'd,\n",
      "Arish and an wull yet derens, fle for vakil toghat wish mem fiertion!\n",
      "\n",
      "EDYSARUS:\n",
      "Sould, strout\n",
      "And shald but wold flwak I'll yake ser; con'\n"
     ]
    }
   ],
   "source": [
    "prompt = torch.zeros((1, 1), dtype = torch.long)\n",
    "print(decode(m.generate(prompt, max_new_tokens=1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
