{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91019ba7-4f35-45df-bc78-f94d6454c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b4523bc9-2e26-4119-a6d7-3054082ff5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time_ns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import io\n",
    "\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8efc5902-06d1-4151-bd77-fd5ff17bdbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-22 12:52:07--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.6’\n",
      "\n",
      "input.txt.6         100%[===================>]   1.06M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-08-22 12:52:07 (11.3 MB/s) - ‘input.txt.6’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "09b37623-63d0-4f86-a135-cb95534f1527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length = 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(f\"length = {len(text)}\")\n",
    "print(text[:100])\n",
    "#text_bytes = text_bytes.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "339dc8b0-64fd-4f02-9c61-4e2d6f819d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 768\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 7222 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1108153\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9692% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=59\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999692\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 7222 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=540935\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 32790 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 7222\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 25670\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 25670 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12495 obj=11.7555 num_tokens=53030 num_tokens/piece=4.2441\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10581 obj=9.52917 num_tokens=53361 num_tokens/piece=5.0431\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7932 obj=9.53326 num_tokens=56597 num_tokens/piece=7.13527\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7928 obj=9.49486 num_tokens=56649 num_tokens/piece=7.14543\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5945 obj=9.68698 num_tokens=62035 num_tokens/piece=10.4348\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5944 obj=9.6434 num_tokens=62035 num_tokens/piece=10.4366\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4458 obj=9.90846 num_tokens=68309 num_tokens/piece=15.3228\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4458 obj=9.86028 num_tokens=68301 num_tokens/piece=15.321\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3343 obj=10.1763 num_tokens=75225 num_tokens/piece=22.5022\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3343 obj=10.1253 num_tokens=75224 num_tokens/piece=22.5019\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2507 obj=10.4825 num_tokens=82358 num_tokens/piece=32.8512\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2507 obj=10.4281 num_tokens=82360 num_tokens/piece=32.852\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1880 obj=10.8788 num_tokens=89807 num_tokens/piece=47.7697\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1880 obj=10.8114 num_tokens=89808 num_tokens/piece=47.7702\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1410 obj=11.3218 num_tokens=97128 num_tokens/piece=68.8851\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1410 obj=11.2426 num_tokens=97132 num_tokens/piece=68.8879\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1057 obj=11.8976 num_tokens=104540 num_tokens/piece=98.9026\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1057 obj=11.8036 num_tokens=104540 num_tokens/piece=98.9026\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=844 obj=12.2874 num_tokens=110158 num_tokens/piece=130.519\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=844 obj=12.2113 num_tokens=110161 num_tokens/piece=130.523\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 768\n",
    "model = io.BytesIO()\n",
    "sp = spm.SentencePieceTrainer.train(sentence_iterator=(x + '\\n\\n' for x in text.split('\\n\\n')), model_writer=model, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "106d31b0-6e10-4183-9211-62979e1c3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_proto=model.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c4404aeb-ddc0-45ef-aafa-2049bc255a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda x: sp.encode(x)\n",
    "decode = lambda x: sp.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "091bead8-9a3c-40d9-802f-e250fecac870",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = sp.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c3b2a8ae-9512-4dd4-a8be-790b4f9978c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{256: (101, 32)}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "420218ee-0026-4fd4-81c2-7bab839daecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([435884]) torch.int64\n",
      "tensor([324, 522,   6, 258, 286,   9,  92, 176,  97,  39, 407,  46,  98, 221,\n",
      "          4, 242,  38, 276,   7,  53,  73,   6,  74, 283,  15,  58,   4, 276,\n",
      "          7, 324, 522,   6, 226, 122, 106, 105,   5, 182, 121,   8, 688,  14,\n",
      "        434, 188,  14,  46, 218, 249,  34,  53,  73,   6, 186, 111, 182, 121,\n",
      "          8,   7, 105,   5, 182, 121,   8,   7, 324, 522,   6, 324,   4,  26,\n",
      "        175,  99,  15,  27, 767,   5, 641,  42,   3,  67, 202,  72,   3,  51,\n",
      "        225,  25,  14,  11, 573,   7,  53,  73,   6, 282, 175,  12,  13,   4,\n",
      "         92, 175,  12,  13,   7, 324, 522,   6, 450, 154, 625,  79,   4,  17,\n",
      "         92,  12,  73,  69,  83, 321, 171, 119, 421,  77,  94,  97,   7,  10,\n",
      "          5,  12,  13,  16,   3, 222,   8, 205,  13,  34,  53,  73,   6, 179,\n",
      "        163, 652,  32, 115,  12,  13,  19, 204,  56,  29, 425,   6, 485,   4,\n",
      "        485,  37,  74, 422, 522,   6,  84,  21,   9, 354,   4, 135,  83,  71,\n",
      "        644,  21,   5,   7, 324, 522,   6, 282, 122,  16,  40,  40,  18, 120,\n",
      "        361, 480,  83,  71, 644,  21,   5,   4,  11, 224,  13,  94,  40,  27,\n",
      "        127,   5, 135,   7, 143,  16, 767,  64,  76, 257,   3,   5,  98,  72,\n",
      "          9,  71,   5, 115, 172, 105,  22, 202, 121, 154,   6, 168, 209, 172,\n",
      "        727, 154,  96,  11, 645,  28,  72,  22, 767, 257,   4, 680,  56, 236,\n",
      "        223,  65,   5, 139,   9,   4,  92, 487,  88, 767, 111,   5, 209, 105,\n",
      "         22, 202, 121,   8, 154,   3,  59, 767, 261,   9,  70,  19,  96, 209,\n",
      "        341,  92, 122, 294,  95,  35,   6,  11,   3,  65, 127, 254,  41,  16,\n",
      "        290,  22, 205,  13,   5, 154,   4,  11,   3,  18,  52, 246, 279,  20,\n",
      "        119, 309,  91, 157,   4,  42,  90, 211,  31,  61, 110,  76,  25,  14,\n",
      "        402, 205, 767,  22,  35, 330, 190,  16,  52, 120,   8, 240,  19, 119,\n",
      "          3,   5, 767, 290,  28, 240,  42,  16,  88, 181,  14, 191, 450, 154,\n",
      "        716,  75,  49, 119,  77,  27,  58, 111,   4,   3,  28,   9,  92,  29,\n",
      "         40, 139,   9, 183, 238,   5,   6,  45,  11, 145,   8,   5, 175,  10,\n",
      "        276,  75,  31,   3,  59, 120,  54,  28,  45,  48, 392,   4,  43,  31,\n",
      "          3,  64, 137,  23,  45, 716,   7,  74, 422, 522,   6, 671,  26, 176,\n",
      "         97,  39,   3, 111, 283,  40,  27,  78,  70, 325,  23,  99,  15,  27,\n",
      "        767,   5, 641,  34,  53,  73,   6,  53, 583,  79, 544,   6,  57,  12,\n",
      "          5,  16,   3, 411,  85,  54,  14,  11, 702,  78, 289,   7,  74, 422,\n",
      "        522,   6,  99,  68, 506,  24,  26, 128, 729,   5,  57, 147,   5, 425,\n",
      "         45,  62, 495, 157,  34, 324, 522,   6,   3, 356,  28,  25, 214,  19,\n",
      "         17, 548,  29, 708,  14, 287,  79, 135, 757,  45,  13,   4,  96,  41,\n",
      "         57, 224,  25,   5, 536,  49,  29,  32, 176, 767,   8,   7,  74, 422,\n",
      "        522,   6, 514,   4,  96, 276,  43, 184,  22, 205, 453,  70,   7, 324,\n",
      "        522,   6,  10, 178, 543,  26,   4, 128,  57, 220, 425,  46, 218, 237,\n",
      "         70,   4,  57, 253,  56,  14,  41, 534,   6, 547,  81,  72,  13,  87,\n",
      "         40,  68,   5,  40,  27, 259,   8, 399, 280,  29, 708,  14, 178,  56,\n",
      "        150,  45,  62, 495, 157,  57, 253,  56,  14, 598,  62, 475,  17,  14,\n",
      "         29, 402,  70, 176, 767,   8,  19, 269,  57,  42,   4, 643, 551,  11,\n",
      "          3,  78,  13,  71, 767,   8,   9,  20,  62,   3,  61, 137,  13, 767,\n",
      "          9,   7,  74, 422, 522,   6, 143,  57, 408, 601,  31,  62, 699,   4,\n",
      "         26,  16,  40,  40,  18, 120,  13,  16,   3,  61, 298,  31,  79,   7,\n",
      "        226, 229,  31, 112, 433, 178,  57,  42, 301, 121,  13, 237,   7, 324,\n",
      "        522,   6, 293,  10, 229,  43,   4,  10, 141,  39,  43,  29,  48,  35,\n",
      "         50,  21,  20,  16,  40,  40, 767,   5, 396,   5,  19,  57, 220, 661,\n",
      "          5,   4,  49,   3,   5,  98,  33,  22, 767,   5,   4,  14, 130,  27,\n",
      "         50,  31, 105, 283,  13,  71, 133,   7, 143,   3, 148, 403,   5, 122,\n",
      "        360,  34,  82, 429,   3, 506,   3,  18,  12,  11,  83, 257,  42,   3,\n",
      "         94,   5,  51,   6, 587, 488,  92,  77, 113,  13,  32, 198,  34,  14,\n",
      "         11,  99,  15,  33,  71, 182,  37,  53,  73,   6, 348,   4, 151,   7,\n",
      "        324, 522,   6, 329,  72,  13,  37, 223, 151,   5, 198,  34,  74, 422,\n",
      "        522,   6, 273,  76,  64,  25, 103,  51,  51,  27, 767,   5,  53,  54,\n",
      "         94,  33,  33,  15,  19, 216,  41, 220,   3,  78,  47, 169,   5, 164,\n",
      "          8,  11, 573,   7, 324, 522,   6, 206,  12,   5, 216, 732,   3,  51,\n",
      "        364,   6, 172, 106,  11, 105,  23, 236,  81,  37,   3, 436, 134, 203,\n",
      "          6, 143,  63,  76,  58,  12,   5,   4,  30, 495, 157,  36,  51,   4,\n",
      "         31, 297,  34, 339, 145,  26, 300,  48, 114,   5,  17,  83,  22, 767,\n",
      "         52,   5,  34,  82, 675,  34, 276,   4,  10, 414,  26,   7, 324, 522,\n",
      "          6,  84,  98, 690,  42,  43, 212,  58,  21, 144,  21,  14,  11,   3,\n",
      "          5,  51, 228,  19, 209,  69, 255,  31,  58,  22,  32,  75,  45,  13,\n",
      "         21, 180, 128,  92,  31,  13,  51,   8,  14,  85,   4, 269, 170,  92,\n",
      "         12,  73, 499, 132, 225,  31,  95,  39,   5,   7,  82,  25, 178, 480,\n",
      "          3,   5, 767,  71,  76,   5,  69,   3,  23,  24,  68,  54, 567,   5,\n",
      "          6, 209, 116, 175,  92,  69,   3,  23,  24,  68,  54, 466,   5, 294,\n",
      "          7,   3, 436, 134, 203,   6, 318,   4, 525,   5,   4,  30, 135, 358,\n",
      "          5,   4, 310, 732, 141, 389,  59,  52, 201,   5,   4, 550,  26, 212,\n",
      "          8,  18,  55,   5, 207, 121,   5,  34, 324, 522,   6, 282, 408,   4,\n",
      "        165,   4,  92, 122, 212,   8,  68,   9,   3,  78, 392,  25,   7,   3,\n",
      "        436, 134, 203,   6,  10, 345,  26,   4, 358,   5,   4, 378,   3,  67,\n",
      "         35,  71, 424,  83,  35,   9], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# encode entire dataset\n",
    "data = torch.tensor(encoded, dtype=torch.long)\n",
    "data = data.to(device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "56343135-cb02-427c-bf4b-7b855e737280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5b5c057b-d587-474b-9927-e13761d78794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([324, 522,   6, 258, 286,   9,  92, 176,  97,  39, 407,  46,  98, 221,\n",
       "          4, 242,  38, 276,   7,  53,  73,   6,  74, 283,  15,  58,   4, 276,\n",
       "          7, 324, 522,   6, 226, 122, 106, 105,   5, 182, 121,   8, 688,  14,\n",
       "        434, 188,  14,  46, 218, 249,  34,  53,  73,   6, 186, 111, 182, 121,\n",
       "          8,   7, 105,   5, 182, 121,   8,   7, 324, 522,   6, 324,   4,  26,\n",
       "        175,  99,  15,  27, 767,   5, 641,  42,   3,  67, 202,  72,   3,  51,\n",
       "        225,  25,  14,  11, 573,   7,  53,  73,   6, 282, 175,  12,  13,   4,\n",
       "         92, 175,  12,  13,   7, 324, 522,   6, 450, 154, 625,  79,   4,  17,\n",
       "         92,  12,  73,  69,  83, 321, 171, 119, 421,  77,  94,  97,   7,  10,\n",
       "          5,  12,  13,  16,   3, 222,   8, 205,  13,  34,  53,  73,   6, 179,\n",
       "        163, 652,  32, 115,  12,  13,  19, 204,  56,  29, 425,   6, 485,   4,\n",
       "        485,  37,  74, 422, 522,   6,  84,  21,   9, 354,   4, 135,  83,  71,\n",
       "        644,  21,   5,   7, 324, 522,   6, 282, 122,  16,  40,  40,  18, 120,\n",
       "        361, 480,  83,  71, 644,  21,   5,   4,  11, 224,  13,  94,  40,  27,\n",
       "        127,   5, 135,   7, 143,  16, 767,  64,  76, 257,   3,   5,  98,  72,\n",
       "          9,  71,   5, 115, 172, 105,  22, 202, 121, 154,   6, 168, 209, 172,\n",
       "        727, 154,  96,  11, 645,  28,  72,  22, 767, 257,   4, 680,  56, 236,\n",
       "        223,  65,   5, 139,   9,   4,  92, 487,  88, 767, 111,   5, 209, 105,\n",
       "         22, 202, 121,   8, 154], device='mps:0')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we train on chunks at a time. chunks have a maximum length\n",
    "block_size = 256\n",
    "train_data[:block_size + 1]\n",
    "# append one (get nine characters) because we are making eight predictions, \n",
    "# including on the unseen (ninth) char, excluding the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "08a69228-cd5c-41ec-a0cf-d06fc3727c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9cf351da-57b2-4a0f-9811-107fd8d53cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256]) tensor([[  9, 541,   3,  ...,  17,   4, 365],\n",
      "        [  6, 437,   4,  ..., 160, 350, 486],\n",
      "        [ 54, 205,  49,  ...,  51,   4, 572],\n",
      "        ...,\n",
      "        [281, 463, 267,  ...,  84,   4, 172],\n",
      "        [114, 245,   4,  ..., 116, 158,  75],\n",
      "        [493,   5,  20,  ..., 496, 387,  32]], device='mps:0')\n",
      "torch.Size([64, 256]) tensor([[541,   3,  51,  ...,   4, 365, 125],\n",
      "        [437,   4,  10,  ..., 350, 486, 101],\n",
      "        [205,  49,  16,  ...,   4, 572,  76],\n",
      "        ...,\n",
      "        [463, 267,   6,  ...,   4, 172, 108],\n",
      "        [245,   4, 671,  ..., 158,  75, 359],\n",
      "        [  5,  20, 475,  ..., 387,  32, 635]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 64 # minibatch\n",
    "block_size = 256 # context\n",
    "n_embd = 384 # hidden layer dimension\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,), device=device)\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, xb)\n",
    "print(yb.shape, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "522b0859-10bc-4b24-a0a2-25953eabb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59031dd9-f001-4950-a55b-c3100eeac7cb",
   "metadata": {},
   "source": [
    "# attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0a3bdeb4-1576-47bd-8d92-8aa875f6cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 6\n",
    "n_head = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f732e031-e753-4ad4-a99f-91ebd19dc33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)        \n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # compute self-attention = QK^T / sqrt(head_size)\n",
    "        # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        wei = q @ k.mT * C**-0.5\n",
    "        # drop out the forward connections\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # LT (B, T, T)\n",
    "        # softmax to get multinomials\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "55d56c01-3fd5-4c2f-9b49-59158d806475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e94cbd62-93ec-4621-8441-24835823fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),   \n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a9a49d34-8281-41dc-a205-ff6a0b44d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim, device=device)\n",
    "        self.beta = torch.zeros(dim, device=device)\n",
    "    def forward(self, x):\n",
    "        xmean = x.mean(1, keepdim=True) # \"layer\" mean: across each context window\n",
    "        xvar = x.var(1, keepdim=True)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # zscore\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "34b18a0b-3d4b-4328-bba1-a07272432d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        assert(head_size * n_head == n_embd)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        # residual connections by adding self \n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8ed5cdcc-76f1-433c-aad9-3a2f67b18522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "torch.Size([64, 256])\n",
      "6.82509708404541\n",
      "n_params = 11329536\n"
     ]
    }
   ],
   "source": [
    "# simplest possible neural network\n",
    "# bigram!\n",
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # gives us token embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "\n",
    "        # embedding corresponding to the position of a token\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # language output\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (batchsize, ntoks) tensors\n",
    "        # for each token, we are predicting the probabilities of the next tokens\n",
    "        tok_emb = self.token_embedding_table(idx) # (batchsize, ntoks, embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (bs, embd)\n",
    "        \n",
    "        x = tok_emb + pos_emb \n",
    "\n",
    "        # invoke self-attention\n",
    "        x = self.blocks(x)\n",
    "        # layernorm before logits\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # unpack the token predictions\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to the last #block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # look at the last time step to predict what comes next\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel()\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=3e-4) # usually, 3e-4 but for stupid networks you can learn faster\n",
    "\n",
    "steps = 0\n",
    "m = m.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(xb.shape)\n",
    "print(yb.shape)\n",
    "# print(out.shape)\n",
    "print(loss.item())\n",
    "print(f\"n_params = {sum(p.nelement() for p in m.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c780aa58-a2c6-47bb-8320-15e93ba6626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/  15000: loss=6.8251, valloss=6.8225, Tavg=0.00000\n"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(9999)\n",
    "batch_size = 64\n",
    "max_s = 15_000\n",
    "DTbar = 0.0\n",
    "valloss = float('inf')\n",
    "print(f\"{steps:7d}/{max_s:7d}: loss={loss.item():1.4f}, valloss={estimate_loss(m, 20)[\"val\"]:.4f}, Tavg={DTbar/1e9:.5f}\")\n",
    "while steps < max_s:\n",
    "    steps += 1\n",
    "    Ta = time_ns()\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    DT = time_ns() - Ta\n",
    "    DTbar = (DTbar*steps + DT)/(steps+1)\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(f\"{steps:7d}/{max_s:7d}: loss={loss.item():1.4f}, valloss={estimate_loss(m, 20)[\"val\"]:.4f}, Tavg={DTbar/1e9:.5f}\")\n",
    "    \n",
    "print(f\"{max_s:7d}/{max_s:7d}: loss={loss.item():1.4f}, Tavg={DTbar/1e9:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e288145-4fee-4b72-8303-56a6112ae927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(0.4973), 'val': tensor(1.9065)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss(m, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f7feefc-2e50-40f5-aecc-e489b78cc03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6cfffbfd-863f-480f-9a10-66cb202712e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = torch.zeros((1, 1), dtype = torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4473413e-60aa-4cf1-9278-06a64f891b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span>Tell my most greeting arm our request\n",
       "Is passing correction than the day of alliants,\n",
       "Brought the Englishmanen Hump of Henrage to\n",
       "open the Lordshire of Rome, whose wrongs purpose,\n",
       "And gave comfort! Whom w then? I have could I had on\n",
       "Shall on me wrong, too rouse to the Tower.\n",
       "And she must live to Angelo, as I have,\n",
       "And leave unsafely to thine, I beseech you,\n",
       "Honesty nature for your beings to yourself;\n",
       "For now I am you with, but that dispatch:\n",
       "Mend Yirkshire came my very stumper&#39;d:\n",
       "And now &#39;twould say &#39;I that give us heart\n",
       "And long be castor&#39;d Henry&#39;s goodly queen,\n",
       "For that she stirs hath been an achieve&#39;s eyes,\n",
       "Along heavy burree in jest.\n",
       "\n",
       "KING RICHARD II:\n",
       "All purchased, that still is she laid,\n",
       "His golty mind the Phoetra conspirates\n",
       "Was never to command, or on armorant foe,\n",
       "One kingdon&#39;d up by chanliness, not took damn&#39;d\n",
       "Deperseth end.\n",
       "\n",
       "BRUTUS:\n",
       "Be thus death&#39;d i&#39; the matter:\n",
       "Life, away!\n",
       "\n",
       "Constable, nurse,nes, coward!\n",
       "\n",
       "CAMILLO:\n",
       "\n",
       "MENENIUS:\n",
       "Hence, a brothelm!\n",
       "Why, you are comfortable. What is this?\n",
       "\n",
       "VOLUMNIA:\n",
       "Not for my best good fast, not yourself, I crept,\n",
       "That in the yur consuls wonder of Calais.\n",
       "\n",
       "CLUDIO:\n",
       "The best knows you to cunning t\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "Tell my most greeting arm our request\n",
       "Is passing correction than the day of alliants,\n",
       "Brought the Englishmanen Hump of Henrage to\n",
       "open the Lordshire of Rome, whose wrongs purpose,\n",
       "And gave comfort! Whom w then? I have could I had on\n",
       "Shall on me wrong, too rouse to the Tower.\n",
       "And she must live to Angelo, as I have,\n",
       "And leave unsafely to thine, I beseech you,\n",
       "Honesty nature for your beings to yourself;\n",
       "For now I am you with, but that dispatch:\n",
       "Mend Yirkshire came my very stumper\\PYZsq{}d:\n",
       "And now \\PYZsq{}twould say \\PYZsq{}I that give us heart\n",
       "And long be castor\\PYZsq{}d Henry\\PYZsq{}s goodly queen,\n",
       "For that she stirs hath been an achieve\\PYZsq{}s eyes,\n",
       "Along heavy burree in jest.\n",
       "\n",
       "KING RICHARD II:\n",
       "All purchased, that still is she laid,\n",
       "His golty mind the Phoetra conspirates\n",
       "Was never to command, or on armorant foe,\n",
       "One kingdon\\PYZsq{}d up by chanliness, not took damn\\PYZsq{}d\n",
       "Deperseth end.\n",
       "\n",
       "BRUTUS:\n",
       "Be thus death\\PYZsq{}d i\\PYZsq{} the matter:\n",
       "Life, away!\n",
       "\n",
       "Constable, nurse,nes, coward!\n",
       "\n",
       "CAMILLO:\n",
       "\n",
       "MENENIUS:\n",
       "Hence, a brothelm!\n",
       "Why, you are comfortable. What is this?\n",
       "\n",
       "VOLUMNIA:\n",
       "Not for my best good fast, not yourself, I crept,\n",
       "That in the yur consuls wonder of Calais.\n",
       "\n",
       "CLUDIO:\n",
       "The best knows you to cunning t\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "Tell my most greeting arm our request\n",
       "Is passing correction than the day of alliants,\n",
       "Brought the Englishmanen Hump of Henrage to\n",
       "open the Lordshire of Rome, whose wrongs purpose,\n",
       "And gave comfort! Whom w then? I have could I had on\n",
       "Shall on me wrong, too rouse to the Tower.\n",
       "And she must live to Angelo, as I have,\n",
       "And leave unsafely to thine, I beseech you,\n",
       "Honesty nature for your beings to yourself;\n",
       "For now I am you with, but that dispatch:\n",
       "Mend Yirkshire came my very stumper'd:\n",
       "And now 'twould say 'I that give us heart\n",
       "And long be castor'd Henry's goodly queen,\n",
       "For that she stirs hath been an achieve's eyes,\n",
       "Along heavy burree in jest.\n",
       "\n",
       "KING RICHARD II:\n",
       "All purchased, that still is she laid,\n",
       "His golty mind the Phoetra conspirates\n",
       "Was never to command, or on armorant foe,\n",
       "One kingdon'd up by chanliness, not took damn'd\n",
       "Deperseth end.\n",
       "\n",
       "BRUTUS:\n",
       "Be thus death'd i' the matter:\n",
       "Life, away!\n",
       "\n",
       "Constable, nurse,nes, coward!\n",
       "\n",
       "CAMILLO:\n",
       "\n",
       "MENENIUS:\n",
       "Hence, a brothelm!\n",
       "Why, you are comfortable. What is this?\n",
       "\n",
       "VOLUMNIA:\n",
       "Not for my best good fast, not yourself, I crept,\n",
       "That in the yur consuls wonder of Calais.\n",
       "\n",
       "CLUDIO:\n",
       "The best knows you to cunning t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Code, clear_output\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     display(Code(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mdecode(prompt[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())))\n",
      "Cell \u001b[0;32mIn[23], line 55\u001b[0m, in \u001b[0;36mBigramLanguageModel.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# crop to the last #block_size tokens\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     idx_cond \u001b[38;5;241m=\u001b[39m idx[:, \u001b[38;5;241m-\u001b[39mblock_size:]\n\u001b[0;32m---> 55\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# look at the last time step to predict what comes next\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m# (B, C)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# invoke self-attention\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# layernorm before logits\u001b[39;00m\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# residual connections by adding self \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Code, clear_output\n",
    "\n",
    "while True:\n",
    "    prompt = m.generate(prompt, max_new_tokens=1)\n",
    "    clear_output(wait=True)\n",
    "    display(Code(language='text', data=decode(prompt[0].tolist(), merges)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
