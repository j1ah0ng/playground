{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efc5902-06d1-4151-bd77-fd5ff17bdbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-17 11:25:58--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.56MB/s    in 0.4s    \n",
      "\n",
      "2024-08-17 11:25:59 (2.56 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b37623-63d0-4f86-a135-cb95534f1527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length = 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(f\"length = {len(text)}\")\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb992240-6790-461e-a67e-c6c3d05eeed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd341b5c-0abf-41e0-ab5a-b61719488b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# character-level language model\n",
    "# we tokenize based on characters\n",
    "\n",
    "# encode chars -> ints\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420218ee-0026-4fd4-81c2-7bab839daecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# encode entire dataset\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "device = torch.device(\"mps\")\n",
    "data = data.to(device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56343135-cb02-427c-bf4b-7b855e737280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5c057b-d587-474b-9927-e13761d78794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35], device='mps:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we train on chunks at a time. chunks have a maximum length\n",
    "block_size = 256\n",
    "train_data[:block_size + 1]\n",
    "# append one (get nine characters) because we are making eight predictions, \n",
    "# including on the unseen (ninth) char, excluding the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a69228-cd5c-41ec-a0cf-d06fc3727c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf351da-57b2-4a0f-9811-107fd8d53cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256]) tensor([[30, 37,  1,  ..., 52, 42, 43],\n",
      "        [53, 59, 56,  ..., 45,  1, 53],\n",
      "        [ 1, 57, 46,  ..., 46, 47, 57],\n",
      "        ...,\n",
      "        [39, 58,  1,  ..., 53, 58,  1],\n",
      "        [47, 52,  1,  ...,  1, 51, 39],\n",
      "        [ 5, 58, 47,  ..., 32, 46, 43]], device='mps:0')\n",
      "torch.Size([64, 256]) tensor([[37,  1, 34,  ..., 42, 43, 42],\n",
      "        [59, 56,  1,  ...,  1, 53, 44],\n",
      "        [57, 46, 53,  ..., 47, 57,  1],\n",
      "        ...,\n",
      "        [58,  1, 51,  ..., 58,  1, 40],\n",
      "        [52,  1, 58,  ..., 51, 39, 63],\n",
      "        [58, 47, 57,  ..., 46, 43, 52]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 64 # minibatch\n",
    "block_size = 256 # context\n",
    "n_embd = 384 # hidden layer dimension\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,), device=device)\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, xb)\n",
    "print(yb.shape, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522b0859-10bc-4b24-a0a2-25953eabb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45e9ba6b-7876-497b-86a7-896a9b0c8146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "torch.Size([64, 256])\n",
      "4.5627923011779785\n",
      "n_params = 148289\n"
     ]
    }
   ],
   "source": [
    "# simplest possible neural network\n",
    "# bigram!\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # gives us token embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "\n",
    "        # embedding corresponding to the position of a token\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "        \n",
    "        # \"language model head\"\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (batchsize, ntoks) tensors\n",
    "        # for each token, we are predicting the probabilities of the next tokens\n",
    "        tok_emb = self.token_embedding_table(idx) # (batchsize, ntoks, embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (bs, embd)\n",
    "        \n",
    "        x = tok_emb + pos_emb \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # unpack the token predictions\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to the last #block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # look at the last time step to predict what comes next\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = Bigram()\n",
    "logits, loss = m(xb, yb)\n",
    "print(xb.shape)\n",
    "print(yb.shape)\n",
    "# print(out.shape)\n",
    "print(loss.item())\n",
    "print(f\"n_params = {sum(p.nelement() for p in m.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d017d2-1a17-403d-b68d-8b49015cd049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.174387269895637)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expected loss should be random -- prob is uniform\n",
    "import numpy as np\n",
    "uniform_prob = 1 / vocab_size\n",
    "-np.log(uniform_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60400d9f-d9d3-4b5a-a989-47168b7b3e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U3mpd!Jj-fxcWkPAQbRe.OOOdUfZE&ewKyjk$cH-k3VCOIi'?gozgZeA\n",
      "pCOrsvMkBcAOyDXOHHN,j&?ofeOAvrYotKyL?DJ&JbL\n"
     ]
    }
   ],
   "source": [
    "# the batch. holds a zero. kicks off the generation here\n",
    "idx = torch.zeros((1, 1), dtype = torch.long, device=device)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bda31630-1a00-4ed4-9aa7-02d4a9afeebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b4b36948-51b1-46ab-8b41-9217bd3ab18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/  10000: loss=4.5600, Tavg=0.28380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m steps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10_000\u001b[39m):\n\u001b[1;32m      6\u001b[0m     Ta \u001b[38;5;241m=\u001b[39m time_ns()\n\u001b[0;32m----> 7\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m m(xb, yb)\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[155], line 9\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m train_data \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m val_data\n\u001b[1;32m      8\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m block_size, (batch_size,), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 9\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i : i \u001b[38;5;241m+\u001b[39m block_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m : i \u001b[38;5;241m+\u001b[39m block_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # usually, 3e-4 but for stupid networks you can learn faster\n",
    "batch_size = 32\n",
    "DTbar = 0\n",
    "\n",
    "for steps in range(10_000):\n",
    "    Ta = time_ns()\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    DT = time_ns() - Ta\n",
    "    DTbar = (DTbar*steps + DT)/(steps+1)\n",
    "\n",
    "    if steps % 1_000 == 0:\n",
    "        print(f\"{steps:7d}/{10_000:7d}: loss={loss.item():1.4f}, Tavg={DTbar/1e9:.5f}\")\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6f93d-7058-436f-ade5-bd6ae9439ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.zeros((1, 1), dtype = torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f0810-129b-4c95-bdf7-40520f98a8da",
   "metadata": {},
   "source": [
    "# self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "348c05f6-12f9-4523-9098-c3d9446a22ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy ex\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch=4, time=8, channels=2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad578473-8aaa-4e4e-83a7-82163c20f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information should flow forwards only: e.g. token at position n can receive info from positions [0, n-1]\n",
    "# simplest way of doing this is to take an average of all the previous tokens\n",
    "xbow = torch.zeros((B, T, C)) # bag of words, averaged prev tokens\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1b045c1-fb44-4d05-8be1-69ea6d1ffa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7e86fc2-c1ae-4236-9c62-60dd0d781153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# this is inefficient. an just matmul\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7732d3c5-423a-4dec-b7fa-16f2161b0695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  7.],\n",
       "        [ 8., 11.],\n",
       "        [14., 16.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can just build a diagonal matrix\n",
    "at = torch.tril(torch.ones(3, 3)) \n",
    "at @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c38d6f6-be6d-4e39-ad0b-75124fd30d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 7.0000],\n",
       "        [4.0000, 5.5000],\n",
       "        [4.6667, 5.3333]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tril @ (T, B, ...) -> (T, B, ...)\n",
    "# now we can get averages\n",
    "(at / torch.sum(at, 1, keepdim=True)) @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb0c7d91-28d6-4d4f-b746-e2aca74e4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_ones = torch.ones(T, T)\n",
    "T_tril = torch.tril(T_ones)\n",
    "T_avgk = T_tril / torch.sum(T_tril, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c83160a1-468e-43a9-950f-078cb2c254ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3199, -0.0396, -0.0682,  0.0084,  0.0020,  0.0712, -0.1128,  0.2527,\n",
      "        -0.0436, -0.2593, -0.3015,  0.4954,  0.3420,  1.1401, -0.4462,  1.0870,\n",
      "        -0.4071, -0.1641])\n",
      "tensor([-0.3199, -0.0396, -0.0682,  0.0084,  0.0020,  0.0712, -0.1128,  0.2527,\n",
      "        -0.0436, -0.2593, -0.3015,  0.4954,  0.3420,  1.1401, -0.4462,  1.0870,\n",
      "        -0.4071, -0.1641])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "xbow2 = T_avgk @ x\n",
    "print(xbow[xbow != xbow2])\n",
    "print(xbow2[xbow != xbow2])\n",
    "print(torch.allclose(xbow, xbow2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0cfb0a3-9370-4219-9f5e-29e8d3dc328b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can use softmax\n",
    "# it basically tells us how significantly tokens interact\n",
    "# --> \"affinities\" for certain tokens in the past\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular ones\n",
    "wei = torch.zeros((T, T)) # all zeros\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # fill the upper triangular portion with -inf\n",
    "wei = F.softmax(wei, dim=-1) # softmax along every row -> .exp().div(sum(dim=1))\n",
    "torch.allclose(wei @ x, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d186071-8f8a-4687-af4e-25ee002ff0ec",
   "metadata": {},
   "source": [
    "lower triangular multiplication gives us how much affinity a token in the ith row has for the previous rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70848bc3-aec5-4906-a6bf-0d2a06dcfcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy ex\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch=4, time=8, channels=2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular ones\n",
    "wei = torch.zeros((T, T)) # all zeros\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # fill the upper triangular portion with -inf\n",
    "wei = F.softmax(wei, dim=-1) # softmax along every row -> .exp().div(sum(dim=1))\n",
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a49c471-036a-453d-9a58-b3e3f5fb2275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ffe036e9-086c-4db3-ae13-cde022517deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f278c-36f2-449d-ab93-b860539ac59c",
   "metadata": {},
   "source": [
    "every token at every position gives two vectors : a query, and a key. the query vector is \"what am i looking for\", and the key is \"what do i contain\"\n",
    "\n",
    "affinities: dot product between keys and query -> dot product gives weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47ea82bf-b65c-46d0-9643-529cc18429fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy ex\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch=4, time=8, channels=3 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# key learns what the token contains\n",
    "# query is what the token wants\n",
    "\n",
    "# all queries dot all the keys\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T). \n",
    "# We now get a T x T matrix containing the weights\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular ones\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # fill the upper triangular portion with -inf\n",
    "wei = F.softmax(wei, dim=-1) # softmax along every row -> .exp().div(sum(dim=1))\n",
    "\n",
    "# x is \"private info\" to the token\n",
    "# v is the thing that is aggregated between nodes in a given window\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f4aef-9a69-49dc-a4e4-ff5122d6adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that wei has no notion of position\n",
    "# this is why we encode positions also\n",
    "\n",
    "# we also may not need the tril masking: here we use it for autoregressive text generation\n",
    "# for other use cases, you could unmask it\n",
    "\n",
    "# \"self-attention\" is looking within the context as Q, K, V come from x.\n",
    "# \"cross-attention\" is when a separate source of information is used\n",
    "\n",
    "# \"scaled\" attention normalizes by sqrt(head size)\n",
    "# to keep the variance of the weight matrix diffuse and disallow softmax from saturating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d14d1e88-01cb-425d-9ff5-4fe64ac5398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9957)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(B, T, head_size) @ torch.randn(B, T, head_size).mT * (head_size**-.5)).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5395edeb-1d5a-4740-b280-63070a1e5e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "29ab3461-4293-4746-a7c6-69cbd63609bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wei is now data dependent\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "079f072e-7d6b-46c1-a760-6950518f34fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59031dd9-f001-4950-a55b-c3100eeac7cb",
   "metadata": {},
   "source": [
    "# attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a3bdeb4-1576-47bd-8d92-8aa875f6cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 6\n",
    "n_head = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f732e031-e753-4ad4-a99f-91ebd19dc33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)        \n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # compute self-attention = QK^T / sqrt(head_size)\n",
    "        # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        wei = q @ k.mT * C**-0.5\n",
    "        # drop out the forward connections\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # LT (B, T, T)\n",
    "        # softmax to get multinomials\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55d56c01-3fd5-4c2f-9b49-59158d806475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e94cbd62-93ec-4621-8441-24835823fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),   \n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a49d34-8281-41dc-a205-ff6a0b44d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim, device=device)\n",
    "        self.beta = torch.zeros(dim, device=device)\n",
    "    def forward(self, x):\n",
    "        xmean = x.mean(1, keepdim=True) # \"layer\" mean: across each context window\n",
    "        xvar = x.var(1, keepdim=True)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # zscore\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34b18a0b-3d4b-4328-bba1-a07272432d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        assert(head_size * n_head == n_embd)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        # residual connections by adding self \n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ed5cdcc-76f1-433c-aad9-3a2f67b18522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "torch.Size([64, 256])\n",
      "4.389363765716553\n",
      "n_params = 10788929\n"
     ]
    }
   ],
   "source": [
    "# simplest possible neural network\n",
    "# bigram!\n",
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # gives us token embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "\n",
    "        # embedding corresponding to the position of a token\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd, device=device)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # language output\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (batchsize, ntoks) tensors\n",
    "        # for each token, we are predicting the probabilities of the next tokens\n",
    "        tok_emb = self.token_embedding_table(idx) # (batchsize, ntoks, embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (bs, embd)\n",
    "        \n",
    "        x = tok_emb + pos_emb \n",
    "\n",
    "        # invoke self-attention\n",
    "        x = self.blocks(x)\n",
    "        # layernorm before logits\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # unpack the token predictions\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop to the last #block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # look at the last time step to predict what comes next\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel()\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=3e-4) # usually, 3e-4 but for stupid networks you can learn faster\n",
    "\n",
    "steps = 0\n",
    "m = m.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(xb.shape)\n",
    "print(yb.shape)\n",
    "# print(out.shape)\n",
    "print(loss.item())\n",
    "print(f\"n_params = {sum(p.nelement() for p in m.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c780aa58-a2c6-47bb-8320-15e93ba6626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2080/  15000: loss=1.4147, valloss=1.5672, Tavg=0.00000\n",
      "   2090/  15000: loss=1.3972, valloss=1.5757, Tavg=0.00358\n",
      "   2100/  15000: loss=1.3903, valloss=1.5774, Tavg=0.00705\n",
      "   2110/  15000: loss=1.4202, valloss=1.5646, Tavg=0.01049\n",
      "   2120/  15000: loss=1.4031, valloss=1.5801, Tavg=0.01390\n",
      "   2130/  15000: loss=1.3890, valloss=1.5784, Tavg=0.01729\n",
      "   2140/  15000: loss=1.4034, valloss=1.5662, Tavg=0.02064\n",
      "   2150/  15000: loss=1.4331, valloss=1.5737, Tavg=0.02395\n",
      "   2160/  15000: loss=1.4193, valloss=1.5627, Tavg=0.02724\n",
      "   2170/  15000: loss=1.3971, valloss=1.5730, Tavg=0.03049\n",
      "   2180/  15000: loss=1.3843, valloss=1.5673, Tavg=0.03372\n",
      "   2190/  15000: loss=1.3772, valloss=1.5613, Tavg=0.03691\n",
      "   2200/  15000: loss=1.4180, valloss=1.5633, Tavg=0.04007\n",
      "   2210/  15000: loss=1.4065, valloss=1.5623, Tavg=0.04323\n",
      "   2220/  15000: loss=1.4035, valloss=1.5511, Tavg=0.04635\n",
      "   2230/  15000: loss=1.3814, valloss=1.5567, Tavg=0.04943\n",
      "   2240/  15000: loss=1.4013, valloss=1.5498, Tavg=0.05249\n",
      "   2250/  15000: loss=1.3696, valloss=1.5606, Tavg=0.05552\n",
      "   2260/  15000: loss=1.3657, valloss=1.5439, Tavg=0.05851\n",
      "   2270/  15000: loss=1.4357, valloss=1.5690, Tavg=0.06149\n",
      "   2280/  15000: loss=1.3991, valloss=1.5608, Tavg=0.06445\n",
      "   2290/  15000: loss=1.4075, valloss=1.5619, Tavg=0.06737\n",
      "   2300/  15000: loss=1.3623, valloss=1.5627, Tavg=0.07026\n",
      "   2310/  15000: loss=1.3813, valloss=1.5507, Tavg=0.07312\n",
      "   2320/  15000: loss=1.3717, valloss=1.5504, Tavg=0.07596\n",
      "   2330/  15000: loss=1.3779, valloss=1.5516, Tavg=0.07877\n",
      "   2340/  15000: loss=1.3648, valloss=1.5346, Tavg=0.08157\n",
      "   2350/  15000: loss=1.3941, valloss=1.5352, Tavg=0.08435\n",
      "   2360/  15000: loss=1.3889, valloss=1.5477, Tavg=0.08710\n",
      "   2370/  15000: loss=1.3769, valloss=1.5346, Tavg=0.08983\n",
      "   2380/  15000: loss=1.3534, valloss=1.5417, Tavg=0.09253\n",
      "   2390/  15000: loss=1.3782, valloss=1.5425, Tavg=0.09521\n",
      "   2400/  15000: loss=1.4102, valloss=1.5362, Tavg=0.09787\n",
      "   2410/  15000: loss=1.3312, valloss=1.5343, Tavg=0.10051\n",
      "   2420/  15000: loss=1.3536, valloss=1.5462, Tavg=0.10313\n",
      "   2430/  15000: loss=1.3358, valloss=1.5424, Tavg=0.10573\n",
      "   2440/  15000: loss=1.3551, valloss=1.5303, Tavg=0.10832\n",
      "   2450/  15000: loss=1.3779, valloss=1.5434, Tavg=0.11088\n",
      "   2460/  15000: loss=1.3865, valloss=1.5438, Tavg=0.11342\n",
      "   2470/  15000: loss=1.3585, valloss=1.5370, Tavg=0.11593\n",
      "   2480/  15000: loss=1.3803, valloss=1.5363, Tavg=0.11842\n",
      "   2490/  15000: loss=1.3655, valloss=1.5403, Tavg=0.12089\n",
      "   2500/  15000: loss=1.3613, valloss=1.5270, Tavg=0.12334\n",
      "   2510/  15000: loss=1.3943, valloss=1.5177, Tavg=0.12577\n",
      "   2520/  15000: loss=1.3541, valloss=1.5351, Tavg=0.12819\n",
      "   2530/  15000: loss=1.3635, valloss=1.5223, Tavg=0.13059\n",
      "   2540/  15000: loss=1.3561, valloss=1.5157, Tavg=0.13297\n",
      "   2550/  15000: loss=1.3465, valloss=1.5180, Tavg=0.13533\n",
      "   2560/  15000: loss=1.3318, valloss=1.5326, Tavg=0.13766\n",
      "   2570/  15000: loss=1.3599, valloss=1.5234, Tavg=0.13997\n",
      "   2580/  15000: loss=1.3426, valloss=1.5339, Tavg=0.14227\n",
      "   2590/  15000: loss=1.3754, valloss=1.5321, Tavg=0.14454\n",
      "   2600/  15000: loss=1.3516, valloss=1.5216, Tavg=0.14681\n",
      "   2610/  15000: loss=1.3426, valloss=1.5288, Tavg=0.14906\n",
      "   2620/  15000: loss=1.3358, valloss=1.5225, Tavg=0.15130\n",
      "   2630/  15000: loss=1.3160, valloss=1.5323, Tavg=0.15352\n",
      "   2640/  15000: loss=1.3799, valloss=1.5299, Tavg=0.15572\n",
      "   2650/  15000: loss=1.3127, valloss=1.5382, Tavg=0.15789\n",
      "   2660/  15000: loss=1.3654, valloss=1.5406, Tavg=0.16004\n",
      "   2670/  15000: loss=1.3219, valloss=1.5306, Tavg=0.16217\n",
      "   2680/  15000: loss=1.3153, valloss=1.5091, Tavg=0.16430\n",
      "   2690/  15000: loss=1.3271, valloss=1.5250, Tavg=0.16642\n",
      "   2700/  15000: loss=1.3236, valloss=1.5253, Tavg=0.16853\n",
      "   2710/  15000: loss=1.3549, valloss=1.5259, Tavg=0.17063\n",
      "   2720/  15000: loss=1.3463, valloss=1.5319, Tavg=0.17270\n",
      "   2730/  15000: loss=1.3461, valloss=1.5144, Tavg=0.17476\n",
      "   2740/  15000: loss=1.3550, valloss=1.5235, Tavg=0.17680\n",
      "   2750/  15000: loss=1.3428, valloss=1.5256, Tavg=0.17882\n",
      "   2760/  15000: loss=1.3126, valloss=1.5274, Tavg=0.18083\n",
      "   2770/  15000: loss=1.3305, valloss=1.5231, Tavg=0.18282\n",
      "   2780/  15000: loss=1.3524, valloss=1.5219, Tavg=0.18480\n",
      "   2790/  15000: loss=1.3256, valloss=1.5273, Tavg=0.18679\n",
      "   2800/  15000: loss=1.3238, valloss=1.5244, Tavg=0.18875\n",
      "   2810/  15000: loss=1.3298, valloss=1.5184, Tavg=0.19069\n",
      "   2820/  15000: loss=1.3212, valloss=1.5112, Tavg=0.19262\n",
      "   2830/  15000: loss=1.3279, valloss=1.5107, Tavg=0.19454\n",
      "   2840/  15000: loss=1.3328, valloss=1.5216, Tavg=0.19644\n",
      "   2850/  15000: loss=1.3502, valloss=1.5151, Tavg=0.19832\n",
      "   2860/  15000: loss=1.3264, valloss=1.5183, Tavg=0.20019\n",
      "   2870/  15000: loss=1.3108, valloss=1.5219, Tavg=0.20206\n",
      "   2880/  15000: loss=1.3410, valloss=1.5169, Tavg=0.20390\n",
      "   2890/  15000: loss=1.3211, valloss=1.5144, Tavg=0.20574\n",
      "   2900/  15000: loss=1.3264, valloss=1.5158, Tavg=0.20757\n",
      "   2910/  15000: loss=1.3119, valloss=1.5025, Tavg=0.20938\n",
      "   2920/  15000: loss=1.3410, valloss=1.5113, Tavg=0.21118\n",
      "   2930/  15000: loss=1.2977, valloss=1.5097, Tavg=0.21296\n",
      "   2940/  15000: loss=1.3174, valloss=1.5290, Tavg=0.21473\n",
      "   2950/  15000: loss=1.3175, valloss=1.5103, Tavg=0.21648\n",
      "   2960/  15000: loss=1.3036, valloss=1.5179, Tavg=0.21823\n",
      "   2970/  15000: loss=1.3046, valloss=1.5207, Tavg=0.21997\n",
      "   2980/  15000: loss=1.3071, valloss=1.4992, Tavg=0.22169\n",
      "   2990/  15000: loss=1.2997, valloss=1.5094, Tavg=0.22340\n",
      "   3000/  15000: loss=1.2986, valloss=1.5100, Tavg=0.22510\n",
      "   3010/  15000: loss=1.3260, valloss=1.5078, Tavg=0.22680\n",
      "   3020/  15000: loss=1.2757, valloss=1.5030, Tavg=0.22848\n",
      "   3030/  15000: loss=1.3262, valloss=1.5160, Tavg=0.23015\n",
      "   3040/  15000: loss=1.3307, valloss=1.4987, Tavg=0.23181\n",
      "   3050/  15000: loss=1.3243, valloss=1.5143, Tavg=0.23346\n",
      "   3060/  15000: loss=1.3086, valloss=1.5070, Tavg=0.23510\n",
      "   3070/  15000: loss=1.2757, valloss=1.5244, Tavg=0.23672\n",
      "   3080/  15000: loss=1.3174, valloss=1.5057, Tavg=0.23834\n",
      "   3090/  15000: loss=1.3189, valloss=1.5177, Tavg=0.23995\n",
      "   3100/  15000: loss=1.2958, valloss=1.5136, Tavg=0.24154\n",
      "   3110/  15000: loss=1.2952, valloss=1.5002, Tavg=0.24313\n",
      "   3120/  15000: loss=1.3219, valloss=1.5196, Tavg=0.24470\n",
      "   3130/  15000: loss=1.2464, valloss=1.5141, Tavg=0.24626\n",
      "   3140/  15000: loss=1.3028, valloss=1.5031, Tavg=0.24781\n",
      "   3150/  15000: loss=1.2999, valloss=1.5193, Tavg=0.24935\n",
      "   3160/  15000: loss=1.3265, valloss=1.4980, Tavg=0.25089\n",
      "   3170/  15000: loss=1.3413, valloss=1.5033, Tavg=0.25242\n",
      "   3180/  15000: loss=1.3019, valloss=1.5035, Tavg=0.25394\n",
      "   3190/  15000: loss=1.2829, valloss=1.5168, Tavg=0.25544\n",
      "   3200/  15000: loss=1.3053, valloss=1.5098, Tavg=0.25694\n",
      "   3210/  15000: loss=1.2779, valloss=1.4979, Tavg=0.25844\n",
      "   3220/  15000: loss=1.2987, valloss=1.4988, Tavg=0.25990\n",
      "   3230/  15000: loss=1.2686, valloss=1.5209, Tavg=0.26136\n",
      "   3240/  15000: loss=1.2726, valloss=1.5034, Tavg=0.26282\n",
      "   3250/  15000: loss=1.2620, valloss=1.5112, Tavg=0.26427\n",
      "   3260/  15000: loss=1.3122, valloss=1.5110, Tavg=0.26572\n",
      "   3270/  15000: loss=1.2904, valloss=1.5027, Tavg=0.26716\n",
      "   3280/  15000: loss=1.2779, valloss=1.5030, Tavg=0.26859\n",
      "   3290/  15000: loss=1.2970, valloss=1.5002, Tavg=0.27001\n",
      "   3300/  15000: loss=1.2840, valloss=1.5030, Tavg=0.27142\n",
      "   3310/  15000: loss=1.3118, valloss=1.5011, Tavg=0.27282\n",
      "   3320/  15000: loss=1.3004, valloss=1.4983, Tavg=0.27420\n",
      "   3330/  15000: loss=1.3057, valloss=1.5096, Tavg=0.27557\n",
      "   3340/  15000: loss=1.2821, valloss=1.5004, Tavg=0.27694\n",
      "   3350/  15000: loss=1.2837, valloss=1.4892, Tavg=0.27831\n",
      "   3360/  15000: loss=1.2911, valloss=1.4971, Tavg=0.27967\n",
      "   3370/  15000: loss=1.2695, valloss=1.5098, Tavg=0.28103\n",
      "   3380/  15000: loss=1.2939, valloss=1.4915, Tavg=0.28238\n",
      "   3390/  15000: loss=1.2965, valloss=1.4928, Tavg=0.28371\n",
      "   3400/  15000: loss=1.2962, valloss=1.5011, Tavg=0.28505\n",
      "   3410/  15000: loss=1.2744, valloss=1.4970, Tavg=0.28637\n",
      "   3420/  15000: loss=1.2646, valloss=1.4993, Tavg=0.28768\n",
      "   3430/  15000: loss=1.2805, valloss=1.4909, Tavg=0.28898\n",
      "   3440/  15000: loss=1.2535, valloss=1.4893, Tavg=0.29028\n",
      "   3450/  15000: loss=1.2931, valloss=1.5038, Tavg=0.29156\n",
      "   3460/  15000: loss=1.2974, valloss=1.5038, Tavg=0.29284\n",
      "   3470/  15000: loss=1.2608, valloss=1.4995, Tavg=0.29411\n",
      "   3480/  15000: loss=1.2674, valloss=1.4948, Tavg=0.29537\n",
      "   3490/  15000: loss=1.2637, valloss=1.4817, Tavg=0.29663\n",
      "   3500/  15000: loss=1.2849, valloss=1.4969, Tavg=0.29788\n",
      "   3510/  15000: loss=1.2689, valloss=1.4964, Tavg=0.29911\n",
      "   3520/  15000: loss=1.2664, valloss=1.4870, Tavg=0.30035\n",
      "   3530/  15000: loss=1.2601, valloss=1.4883, Tavg=0.30158\n",
      "   3540/  15000: loss=1.2779, valloss=1.4863, Tavg=0.30280\n",
      "   3550/  15000: loss=1.2654, valloss=1.4985, Tavg=0.30400\n",
      "   3560/  15000: loss=1.2554, valloss=1.5059, Tavg=0.30521\n",
      "   3570/  15000: loss=1.2621, valloss=1.4907, Tavg=0.30640\n",
      "   3580/  15000: loss=1.2517, valloss=1.4875, Tavg=0.30759\n",
      "   3590/  15000: loss=1.2485, valloss=1.4840, Tavg=0.30877\n",
      "   3600/  15000: loss=1.2496, valloss=1.5052, Tavg=0.30995\n",
      "   3610/  15000: loss=1.2617, valloss=1.5076, Tavg=0.31112\n",
      "   3620/  15000: loss=1.2591, valloss=1.4975, Tavg=0.31228\n",
      "   3630/  15000: loss=1.2448, valloss=1.4903, Tavg=0.31344\n",
      "   3640/  15000: loss=1.2768, valloss=1.4920, Tavg=0.31459\n",
      "   3650/  15000: loss=1.2809, valloss=1.4866, Tavg=0.31573\n",
      "   3660/  15000: loss=1.2752, valloss=1.4981, Tavg=0.31687\n",
      "   3670/  15000: loss=1.2520, valloss=1.4832, Tavg=0.31800\n",
      "   3680/  15000: loss=1.2142, valloss=1.4883, Tavg=0.31913\n",
      "   3690/  15000: loss=1.2511, valloss=1.4992, Tavg=0.32024\n",
      "   3700/  15000: loss=1.2519, valloss=1.4916, Tavg=0.32136\n",
      "   3710/  15000: loss=1.2558, valloss=1.4900, Tavg=0.32247\n",
      "   3720/  15000: loss=1.2700, valloss=1.4955, Tavg=0.32358\n",
      "   3730/  15000: loss=1.2564, valloss=1.4986, Tavg=0.32468\n",
      "   3740/  15000: loss=1.2666, valloss=1.4880, Tavg=0.32577\n",
      "   3750/  15000: loss=1.2692, valloss=1.4915, Tavg=0.32686\n",
      "   3760/  15000: loss=1.2470, valloss=1.4831, Tavg=0.32794\n",
      "   3770/  15000: loss=1.2585, valloss=1.4975, Tavg=0.32901\n",
      "   3780/  15000: loss=1.2899, valloss=1.4881, Tavg=0.33008\n",
      "   3790/  15000: loss=1.2375, valloss=1.4904, Tavg=0.33115\n",
      "   3800/  15000: loss=1.2676, valloss=1.5030, Tavg=0.33221\n",
      "   3810/  15000: loss=1.2335, valloss=1.4940, Tavg=0.33326\n",
      "   3820/  15000: loss=1.2505, valloss=1.4846, Tavg=0.33431\n",
      "   3830/  15000: loss=1.2768, valloss=1.4818, Tavg=0.33536\n",
      "   3840/  15000: loss=1.2810, valloss=1.4919, Tavg=0.33639\n",
      "   3850/  15000: loss=1.2392, valloss=1.4949, Tavg=0.33742\n",
      "   3860/  15000: loss=1.2504, valloss=1.4861, Tavg=0.33844\n",
      "   3870/  15000: loss=1.2331, valloss=1.4973, Tavg=0.33946\n",
      "   3880/  15000: loss=1.2275, valloss=1.4974, Tavg=0.34048\n",
      "   3890/  15000: loss=1.2218, valloss=1.4864, Tavg=0.34149\n",
      "   3900/  15000: loss=1.2584, valloss=1.4947, Tavg=0.34251\n",
      "   3910/  15000: loss=1.2632, valloss=1.4829, Tavg=0.34351\n",
      "   3920/  15000: loss=1.2574, valloss=1.5019, Tavg=0.34451\n",
      "   3930/  15000: loss=1.2736, valloss=1.4867, Tavg=0.34550\n",
      "   3940/  15000: loss=1.2297, valloss=1.4989, Tavg=0.34649\n",
      "   3950/  15000: loss=1.2363, valloss=1.4869, Tavg=0.34747\n",
      "   3960/  15000: loss=1.2675, valloss=1.4899, Tavg=0.34845\n",
      "   3970/  15000: loss=1.2707, valloss=1.4887, Tavg=0.34941\n",
      "   3980/  15000: loss=1.2086, valloss=1.4991, Tavg=0.35037\n",
      "   3990/  15000: loss=1.2761, valloss=1.4810, Tavg=0.35133\n",
      "   4000/  15000: loss=1.2109, valloss=1.4914, Tavg=0.35229\n",
      "   4010/  15000: loss=1.2157, valloss=1.4879, Tavg=0.35324\n",
      "   4020/  15000: loss=1.2457, valloss=1.4805, Tavg=0.35419\n",
      "   4030/  15000: loss=1.2502, valloss=1.4921, Tavg=0.35513\n",
      "   4040/  15000: loss=1.2338, valloss=1.5023, Tavg=0.35607\n",
      "   4050/  15000: loss=1.2750, valloss=1.4958, Tavg=0.35700\n",
      "   4060/  15000: loss=1.2163, valloss=1.4877, Tavg=0.35792\n",
      "   4070/  15000: loss=1.2279, valloss=1.5049, Tavg=0.35884\n",
      "   4080/  15000: loss=1.1972, valloss=1.4833, Tavg=0.35976\n",
      "   4090/  15000: loss=1.2693, valloss=1.4851, Tavg=0.36068\n",
      "   4100/  15000: loss=1.2262, valloss=1.4955, Tavg=0.36159\n",
      "   4110/  15000: loss=1.2373, valloss=1.4986, Tavg=0.36250\n",
      "   4120/  15000: loss=1.2434, valloss=1.4875, Tavg=0.36340\n",
      "   4130/  15000: loss=1.2159, valloss=1.4896, Tavg=0.36430\n",
      "   4140/  15000: loss=1.2383, valloss=1.4786, Tavg=0.36520\n",
      "   4150/  15000: loss=1.2198, valloss=1.4958, Tavg=0.36609\n",
      "   4160/  15000: loss=1.2369, valloss=1.5020, Tavg=0.36697\n",
      "   4170/  15000: loss=1.2462, valloss=1.4830, Tavg=0.36785\n",
      "   4180/  15000: loss=1.2425, valloss=1.4783, Tavg=0.36873\n",
      "   4190/  15000: loss=1.2342, valloss=1.4850, Tavg=0.36961\n",
      "   4200/  15000: loss=1.2368, valloss=1.4738, Tavg=0.37048\n",
      "   4210/  15000: loss=1.2346, valloss=1.4839, Tavg=0.37134\n",
      "   4220/  15000: loss=1.2219, valloss=1.4806, Tavg=0.37220\n",
      "   4230/  15000: loss=1.2016, valloss=1.4771, Tavg=0.37306\n",
      "   4240/  15000: loss=1.2122, valloss=1.4822, Tavg=0.37390\n",
      "   4250/  15000: loss=1.2090, valloss=1.4684, Tavg=0.37474\n",
      "   4260/  15000: loss=1.2240, valloss=1.4804, Tavg=0.37559\n",
      "   4270/  15000: loss=1.2299, valloss=1.4869, Tavg=0.37645\n",
      "   4280/  15000: loss=1.2346, valloss=1.4893, Tavg=0.37728\n",
      "   4290/  15000: loss=1.2510, valloss=1.4786, Tavg=0.37813\n",
      "   4300/  15000: loss=1.2131, valloss=1.4980, Tavg=0.37897\n",
      "   4310/  15000: loss=1.2318, valloss=1.4753, Tavg=0.37980\n",
      "   4320/  15000: loss=1.2253, valloss=1.4917, Tavg=0.38063\n",
      "   4330/  15000: loss=1.2173, valloss=1.4956, Tavg=0.38145\n",
      "   4340/  15000: loss=1.2244, valloss=1.4948, Tavg=0.38227\n",
      "   4350/  15000: loss=1.2151, valloss=1.4943, Tavg=0.38308\n",
      "   4360/  15000: loss=1.2006, valloss=1.5010, Tavg=0.38390\n",
      "   4370/  15000: loss=1.2269, valloss=1.5053, Tavg=0.38473\n",
      "   4380/  15000: loss=1.2077, valloss=1.4962, Tavg=0.38554\n",
      "   4390/  15000: loss=1.2073, valloss=1.4872, Tavg=0.38636\n",
      "   4400/  15000: loss=1.2210, valloss=1.4850, Tavg=0.38717\n",
      "   4410/  15000: loss=1.2082, valloss=1.5000, Tavg=0.38796\n",
      "   4420/  15000: loss=1.1928, valloss=1.4866, Tavg=0.38875\n",
      "   4430/  15000: loss=1.1877, valloss=1.4920, Tavg=0.38953\n",
      "   4440/  15000: loss=1.2011, valloss=1.4984, Tavg=0.39032\n",
      "   4450/  15000: loss=1.2051, valloss=1.4824, Tavg=0.39109\n",
      "   4460/  15000: loss=1.2342, valloss=1.4953, Tavg=0.39188\n",
      "   4470/  15000: loss=1.1956, valloss=1.4948, Tavg=0.39264\n",
      "   4480/  15000: loss=1.2007, valloss=1.4837, Tavg=0.39340\n",
      "   4490/  15000: loss=1.1954, valloss=1.4752, Tavg=0.39418\n",
      "   4500/  15000: loss=1.2067, valloss=1.4941, Tavg=0.39495\n",
      "   4510/  15000: loss=1.2022, valloss=1.5041, Tavg=0.39571\n",
      "   4520/  15000: loss=1.2080, valloss=1.4865, Tavg=0.39648\n",
      "   4530/  15000: loss=1.2267, valloss=1.4926, Tavg=0.39723\n",
      "   4540/  15000: loss=1.1988, valloss=1.4929, Tavg=0.39798\n",
      "   4550/  15000: loss=1.2053, valloss=1.4750, Tavg=0.39874\n",
      "   4560/  15000: loss=1.1938, valloss=1.4882, Tavg=0.39949\n",
      "   4570/  15000: loss=1.2126, valloss=1.4859, Tavg=0.40023\n",
      "   4580/  15000: loss=1.1850, valloss=1.4729, Tavg=0.40097\n",
      "   4590/  15000: loss=1.1905, valloss=1.5017, Tavg=0.40171\n",
      "   4600/  15000: loss=1.1969, valloss=1.4995, Tavg=0.40245\n",
      "   4610/  15000: loss=1.1771, valloss=1.5012, Tavg=0.40318\n",
      "   4620/  15000: loss=1.1838, valloss=1.4860, Tavg=0.40390\n",
      "   4630/  15000: loss=1.2099, valloss=1.4823, Tavg=0.40461\n",
      "   4640/  15000: loss=1.2053, valloss=1.4897, Tavg=0.40532\n",
      "   4650/  15000: loss=1.1905, valloss=1.4868, Tavg=0.40604\n",
      "   4660/  15000: loss=1.2029, valloss=1.4860, Tavg=0.40676\n",
      "   4670/  15000: loss=1.2078, valloss=1.4992, Tavg=0.40747\n",
      "   4680/  15000: loss=1.1798, valloss=1.4891, Tavg=0.40818\n",
      "   4690/  15000: loss=1.1829, valloss=1.4867, Tavg=0.40890\n",
      "   4700/  15000: loss=1.2196, valloss=1.4878, Tavg=0.40961\n",
      "   4710/  15000: loss=1.2183, valloss=1.4785, Tavg=0.41032\n",
      "   4720/  15000: loss=1.2043, valloss=1.4756, Tavg=0.41100\n",
      "   4730/  15000: loss=1.2046, valloss=1.4850, Tavg=0.41170\n",
      "   4740/  15000: loss=1.2093, valloss=1.4994, Tavg=0.41239\n",
      "   4750/  15000: loss=1.1979, valloss=1.4859, Tavg=0.41309\n",
      "   4760/  15000: loss=1.2077, valloss=1.4979, Tavg=0.41378\n",
      "   4770/  15000: loss=1.1697, valloss=1.5013, Tavg=0.41446\n",
      "   4780/  15000: loss=1.2308, valloss=1.4791, Tavg=0.41514\n",
      "   4790/  15000: loss=1.1741, valloss=1.5013, Tavg=0.41581\n",
      "   4800/  15000: loss=1.1618, valloss=1.4802, Tavg=0.41649\n",
      "   4810/  15000: loss=1.1821, valloss=1.4882, Tavg=0.41715\n",
      "   4820/  15000: loss=1.1982, valloss=1.4758, Tavg=0.41780\n",
      "   4830/  15000: loss=1.2164, valloss=1.4922, Tavg=0.41847\n",
      "   4840/  15000: loss=1.1863, valloss=1.4949, Tavg=0.41914\n",
      "   4850/  15000: loss=1.1807, valloss=1.4776, Tavg=0.41980\n",
      "   4860/  15000: loss=1.1943, valloss=1.4770, Tavg=0.42046\n",
      "   4870/  15000: loss=1.1887, valloss=1.4888, Tavg=0.42111\n",
      "   4880/  15000: loss=1.1650, valloss=1.4930, Tavg=0.42176\n",
      "   4890/  15000: loss=1.1921, valloss=1.4836, Tavg=0.42241\n",
      "   4900/  15000: loss=1.1910, valloss=1.4842, Tavg=0.42306\n",
      "   4910/  15000: loss=1.1304, valloss=1.4781, Tavg=0.42370\n",
      "   4920/  15000: loss=1.2040, valloss=1.4934, Tavg=0.42435\n",
      "   4930/  15000: loss=1.1816, valloss=1.4885, Tavg=0.42499\n",
      "   4940/  15000: loss=1.1659, valloss=1.4739, Tavg=0.42564\n",
      "   4950/  15000: loss=1.1577, valloss=1.4954, Tavg=0.42627\n",
      "   4960/  15000: loss=1.1846, valloss=1.4839, Tavg=0.42691\n",
      "   4970/  15000: loss=1.1748, valloss=1.4862, Tavg=0.42754\n",
      "   4980/  15000: loss=1.1871, valloss=1.4914, Tavg=0.42816\n",
      "   4990/  15000: loss=1.1752, valloss=1.4953, Tavg=0.42878\n",
      "   5000/  15000: loss=1.1654, valloss=1.4868, Tavg=0.42939\n",
      "   5010/  15000: loss=1.1679, valloss=1.4952, Tavg=0.43001\n",
      "   5020/  15000: loss=1.1633, valloss=1.4974, Tavg=0.43063\n",
      "   5030/  15000: loss=1.1663, valloss=1.4975, Tavg=0.43124\n",
      "   5040/  15000: loss=1.1831, valloss=1.4944, Tavg=0.43185\n",
      "   5050/  15000: loss=1.1722, valloss=1.4826, Tavg=0.43247\n",
      "   5060/  15000: loss=1.1739, valloss=1.4914, Tavg=0.43307\n",
      "   5070/  15000: loss=1.2003, valloss=1.4791, Tavg=0.43367\n",
      "   5080/  15000: loss=1.1716, valloss=1.4968, Tavg=0.43427\n",
      "   5090/  15000: loss=1.2068, valloss=1.5004, Tavg=0.43487\n",
      "   5100/  15000: loss=1.1894, valloss=1.4843, Tavg=0.43546\n",
      "   5110/  15000: loss=1.1624, valloss=1.4839, Tavg=0.43605\n",
      "   5120/  15000: loss=1.1705, valloss=1.5046, Tavg=0.43663\n",
      "   5130/  15000: loss=1.1783, valloss=1.4979, Tavg=0.43722\n",
      "   5140/  15000: loss=1.1388, valloss=1.4862, Tavg=0.43780\n",
      "   5150/  15000: loss=1.1715, valloss=1.5070, Tavg=0.43839\n",
      "   5160/  15000: loss=1.1789, valloss=1.4874, Tavg=0.43897\n",
      "   5170/  15000: loss=1.1818, valloss=1.4831, Tavg=0.43955\n",
      "   5180/  15000: loss=1.1508, valloss=1.4914, Tavg=0.44013\n",
      "   5190/  15000: loss=1.1664, valloss=1.4854, Tavg=0.44070\n",
      "   5200/  15000: loss=1.1777, valloss=1.4809, Tavg=0.44127\n",
      "   5210/  15000: loss=1.1796, valloss=1.4949, Tavg=0.44184\n",
      "   5220/  15000: loss=1.1578, valloss=1.4935, Tavg=0.44241\n",
      "   5230/  15000: loss=1.1777, valloss=1.4743, Tavg=0.44298\n",
      "   5240/  15000: loss=1.1823, valloss=1.4838, Tavg=0.44355\n",
      "   5250/  15000: loss=1.1161, valloss=1.4820, Tavg=0.44411\n",
      "   5260/  15000: loss=1.1735, valloss=1.4965, Tavg=0.44468\n",
      "   5270/  15000: loss=1.1631, valloss=1.4732, Tavg=0.44523\n",
      "   5280/  15000: loss=1.1446, valloss=1.4939, Tavg=0.44580\n",
      "   5290/  15000: loss=1.1704, valloss=1.4997, Tavg=0.44635\n",
      "   5300/  15000: loss=1.1343, valloss=1.5041, Tavg=0.44690\n",
      "   5310/  15000: loss=1.1460, valloss=1.4892, Tavg=0.44745\n",
      "   5320/  15000: loss=1.1778, valloss=1.4898, Tavg=0.44799\n",
      "   5330/  15000: loss=1.1346, valloss=1.4852, Tavg=0.44852\n",
      "   5340/  15000: loss=1.1669, valloss=1.4882, Tavg=0.44907\n",
      "   5350/  15000: loss=1.1477, valloss=1.4856, Tavg=0.44962\n",
      "   5360/  15000: loss=1.1560, valloss=1.5042, Tavg=0.45017\n",
      "   5370/  15000: loss=1.1899, valloss=1.4876, Tavg=0.45071\n",
      "   5380/  15000: loss=1.1493, valloss=1.4890, Tavg=0.45123\n",
      "   5390/  15000: loss=1.1598, valloss=1.4741, Tavg=0.45177\n",
      "   5400/  15000: loss=1.1609, valloss=1.5034, Tavg=0.45230\n",
      "   5410/  15000: loss=1.1355, valloss=1.4873, Tavg=0.45283\n",
      "   5420/  15000: loss=1.1537, valloss=1.4914, Tavg=0.45336\n",
      "   5430/  15000: loss=1.1697, valloss=1.4999, Tavg=0.45388\n",
      "   5440/  15000: loss=1.1515, valloss=1.4879, Tavg=0.45441\n",
      "   5450/  15000: loss=1.1354, valloss=1.4874, Tavg=0.45494\n",
      "   5460/  15000: loss=1.1479, valloss=1.4904, Tavg=0.45546\n",
      "   5470/  15000: loss=1.1641, valloss=1.4856, Tavg=0.45597\n",
      "   5480/  15000: loss=1.1750, valloss=1.4982, Tavg=0.45649\n",
      "   5490/  15000: loss=1.1854, valloss=1.4898, Tavg=0.45700\n",
      "   5500/  15000: loss=1.1169, valloss=1.4967, Tavg=0.45751\n",
      "   5510/  15000: loss=1.1751, valloss=1.4916, Tavg=0.45802\n",
      "   5520/  15000: loss=1.1180, valloss=1.4975, Tavg=0.45853\n",
      "   5530/  15000: loss=1.1535, valloss=1.4966, Tavg=0.45904\n",
      "   5540/  15000: loss=1.1689, valloss=1.4976, Tavg=0.45955\n",
      "   5550/  15000: loss=1.1233, valloss=1.4897, Tavg=0.46006\n",
      "   5560/  15000: loss=1.1355, valloss=1.4991, Tavg=0.46057\n",
      "   5570/  15000: loss=1.1681, valloss=1.5105, Tavg=0.46107\n",
      "   5580/  15000: loss=1.1243, valloss=1.4878, Tavg=0.46158\n",
      "   5590/  15000: loss=1.1352, valloss=1.4980, Tavg=0.46207\n",
      "   5600/  15000: loss=1.1701, valloss=1.5033, Tavg=0.46256\n",
      "   5610/  15000: loss=1.1231, valloss=1.4923, Tavg=0.46305\n",
      "   5620/  15000: loss=1.1418, valloss=1.4964, Tavg=0.46353\n",
      "   5630/  15000: loss=1.1528, valloss=1.4791, Tavg=0.46401\n",
      "   5640/  15000: loss=1.1568, valloss=1.4942, Tavg=0.46449\n",
      "   5650/  15000: loss=1.1481, valloss=1.5023, Tavg=0.46498\n",
      "   5660/  15000: loss=1.1606, valloss=1.5033, Tavg=0.46546\n",
      "   5670/  15000: loss=1.1450, valloss=1.5058, Tavg=0.46593\n",
      "   5680/  15000: loss=1.1463, valloss=1.4892, Tavg=0.46641\n",
      "   5690/  15000: loss=1.1423, valloss=1.4892, Tavg=0.46689\n",
      "   5700/  15000: loss=1.1383, valloss=1.5152, Tavg=0.46736\n",
      "   5710/  15000: loss=1.1292, valloss=1.4968, Tavg=0.46783\n",
      "   5720/  15000: loss=1.1270, valloss=1.5058, Tavg=0.46829\n",
      "   5730/  15000: loss=1.1131, valloss=1.4944, Tavg=0.46877\n",
      "   5740/  15000: loss=1.1801, valloss=1.4900, Tavg=0.46923\n",
      "   5750/  15000: loss=1.1095, valloss=1.4940, Tavg=0.46969\n",
      "   5760/  15000: loss=1.1109, valloss=1.4930, Tavg=0.47015\n",
      "   5770/  15000: loss=1.1467, valloss=1.4900, Tavg=0.47061\n",
      "   5780/  15000: loss=1.1651, valloss=1.4867, Tavg=0.47107\n",
      "   5790/  15000: loss=1.1529, valloss=1.4994, Tavg=0.47153\n",
      "   5800/  15000: loss=1.1432, valloss=1.4914, Tavg=0.47199\n",
      "   5810/  15000: loss=1.1200, valloss=1.5001, Tavg=0.47245\n",
      "   5820/  15000: loss=1.1347, valloss=1.4988, Tavg=0.47290\n",
      "   5830/  15000: loss=1.1540, valloss=1.5059, Tavg=0.47335\n",
      "   5840/  15000: loss=1.1280, valloss=1.4944, Tavg=0.47380\n",
      "   5850/  15000: loss=1.1529, valloss=1.4945, Tavg=0.47426\n",
      "   5860/  15000: loss=1.1095, valloss=1.5010, Tavg=0.47470\n",
      "   5870/  15000: loss=1.1351, valloss=1.5047, Tavg=0.47516\n",
      "   5880/  15000: loss=1.1201, valloss=1.5125, Tavg=0.47562\n",
      "   5890/  15000: loss=1.1123, valloss=1.5010, Tavg=0.47607\n",
      "   5900/  15000: loss=1.1611, valloss=1.5090, Tavg=0.47651\n",
      "   5910/  15000: loss=1.1094, valloss=1.5048, Tavg=0.47696\n",
      "   5920/  15000: loss=1.1239, valloss=1.4999, Tavg=0.47740\n",
      "   5930/  15000: loss=1.1360, valloss=1.4976, Tavg=0.47785\n",
      "   5940/  15000: loss=1.1316, valloss=1.5210, Tavg=0.47829\n",
      "   5950/  15000: loss=1.1058, valloss=1.5092, Tavg=0.47872\n",
      "   5960/  15000: loss=1.1550, valloss=1.5026, Tavg=0.47915\n",
      "   5970/  15000: loss=1.1111, valloss=1.5027, Tavg=0.47958\n",
      "   5980/  15000: loss=1.1400, valloss=1.4992, Tavg=0.48002\n",
      "   5990/  15000: loss=1.1287, valloss=1.4997, Tavg=0.48045\n",
      "   6000/  15000: loss=1.1299, valloss=1.5099, Tavg=0.48088\n",
      "   6010/  15000: loss=1.1265, valloss=1.5115, Tavg=0.48131\n",
      "   6020/  15000: loss=1.1406, valloss=1.4952, Tavg=0.48173\n",
      "   6030/  15000: loss=1.1099, valloss=1.4998, Tavg=0.48215\n",
      "   6040/  15000: loss=1.1035, valloss=1.4988, Tavg=0.48257\n",
      "   6050/  15000: loss=1.1121, valloss=1.5008, Tavg=0.48300\n",
      "   6060/  15000: loss=1.1374, valloss=1.5085, Tavg=0.48342\n",
      "   6070/  15000: loss=1.1292, valloss=1.5081, Tavg=0.48385\n",
      "   6080/  15000: loss=1.0903, valloss=1.5098, Tavg=0.48428\n",
      "   6090/  15000: loss=1.0801, valloss=1.5120, Tavg=0.48470\n",
      "   6100/  15000: loss=1.1344, valloss=1.5070, Tavg=0.48512\n",
      "   6110/  15000: loss=1.1117, valloss=1.4975, Tavg=0.48554\n",
      "   6120/  15000: loss=1.1317, valloss=1.5094, Tavg=0.48595\n",
      "   6130/  15000: loss=1.1313, valloss=1.5146, Tavg=0.48636\n",
      "   6140/  15000: loss=1.1082, valloss=1.5288, Tavg=0.48676\n",
      "   6150/  15000: loss=1.1126, valloss=1.5043, Tavg=0.48716\n",
      "   6160/  15000: loss=1.1284, valloss=1.5072, Tavg=0.48757\n",
      "   6170/  15000: loss=1.1173, valloss=1.4966, Tavg=0.48798\n",
      "   6180/  15000: loss=1.1145, valloss=1.5130, Tavg=0.48839\n",
      "   6190/  15000: loss=1.0924, valloss=1.5037, Tavg=0.48879\n",
      "   6200/  15000: loss=1.1475, valloss=1.5140, Tavg=0.48920\n",
      "   6210/  15000: loss=1.1251, valloss=1.5112, Tavg=0.48960\n",
      "   6220/  15000: loss=1.1190, valloss=1.5046, Tavg=0.48999\n",
      "   6230/  15000: loss=1.0914, valloss=1.5006, Tavg=0.49038\n",
      "   6240/  15000: loss=1.0760, valloss=1.5088, Tavg=0.49077\n",
      "   6250/  15000: loss=1.1001, valloss=1.4930, Tavg=0.49117\n",
      "   6260/  15000: loss=1.0822, valloss=1.5173, Tavg=0.49156\n",
      "   6270/  15000: loss=1.1061, valloss=1.5040, Tavg=0.49196\n",
      "   6280/  15000: loss=1.1121, valloss=1.5208, Tavg=0.49235\n",
      "   6290/  15000: loss=1.1123, valloss=1.5180, Tavg=0.49273\n",
      "   6300/  15000: loss=1.0826, valloss=1.4954, Tavg=0.49313\n",
      "   6310/  15000: loss=1.1356, valloss=1.4964, Tavg=0.49351\n",
      "   6320/  15000: loss=1.1066, valloss=1.4997, Tavg=0.49389\n",
      "   6330/  15000: loss=1.0934, valloss=1.5281, Tavg=0.49428\n",
      "   6340/  15000: loss=1.1268, valloss=1.5002, Tavg=0.49466\n",
      "   6350/  15000: loss=1.1081, valloss=1.5053, Tavg=0.49504\n",
      "   6360/  15000: loss=1.1191, valloss=1.5090, Tavg=0.49543\n",
      "   6370/  15000: loss=1.0758, valloss=1.5158, Tavg=0.49581\n",
      "   6380/  15000: loss=1.1179, valloss=1.5063, Tavg=0.49620\n",
      "   6390/  15000: loss=1.1162, valloss=1.5148, Tavg=0.49658\n",
      "   6400/  15000: loss=1.0793, valloss=1.5094, Tavg=0.49695\n",
      "   6410/  15000: loss=1.1034, valloss=1.5127, Tavg=0.49732\n",
      "   6420/  15000: loss=1.1355, valloss=1.5143, Tavg=0.49770\n",
      "   6430/  15000: loss=1.1090, valloss=1.5117, Tavg=0.49807\n",
      "   6440/  15000: loss=1.1227, valloss=1.5057, Tavg=0.49844\n",
      "   6450/  15000: loss=1.1338, valloss=1.5069, Tavg=0.49881\n",
      "   6460/  15000: loss=1.1415, valloss=1.5217, Tavg=0.49918\n",
      "   6470/  15000: loss=1.0785, valloss=1.5216, Tavg=0.49955\n",
      "   6480/  15000: loss=1.0834, valloss=1.5023, Tavg=0.49991\n",
      "   6490/  15000: loss=1.0968, valloss=1.5127, Tavg=0.50028\n",
      "   6500/  15000: loss=1.1073, valloss=1.5204, Tavg=0.50064\n",
      "   6510/  15000: loss=1.1090, valloss=1.5060, Tavg=0.50100\n",
      "   6520/  15000: loss=1.1015, valloss=1.5147, Tavg=0.50137\n",
      "   6530/  15000: loss=1.1176, valloss=1.5150, Tavg=0.50173\n",
      "   6540/  15000: loss=1.1051, valloss=1.4999, Tavg=0.50210\n",
      "   6550/  15000: loss=1.1031, valloss=1.5223, Tavg=0.50245\n",
      "   6560/  15000: loss=1.0705, valloss=1.5108, Tavg=0.50281\n",
      "   6570/  15000: loss=1.1096, valloss=1.5160, Tavg=0.50317\n",
      "   6580/  15000: loss=1.0941, valloss=1.5329, Tavg=0.50352\n",
      "   6590/  15000: loss=1.0767, valloss=1.5193, Tavg=0.50388\n",
      "   6600/  15000: loss=1.1066, valloss=1.5110, Tavg=0.50423\n",
      "   6610/  15000: loss=1.1197, valloss=1.5197, Tavg=0.50458\n",
      "   6620/  15000: loss=1.0938, valloss=1.5170, Tavg=0.50494\n",
      "   6630/  15000: loss=1.1005, valloss=1.4959, Tavg=0.50530\n",
      "   6640/  15000: loss=1.1178, valloss=1.5060, Tavg=0.50564\n",
      "   6650/  15000: loss=1.0886, valloss=1.5097, Tavg=0.50599\n",
      "   6660/  15000: loss=1.0814, valloss=1.5056, Tavg=0.50634\n",
      "   6670/  15000: loss=1.0556, valloss=1.5020, Tavg=0.50669\n",
      "   6680/  15000: loss=1.0851, valloss=1.5228, Tavg=0.50703\n",
      "   6690/  15000: loss=1.0952, valloss=1.5042, Tavg=0.50738\n",
      "   6700/  15000: loss=1.1255, valloss=1.5071, Tavg=0.50772\n",
      "   6710/  15000: loss=1.0921, valloss=1.5349, Tavg=0.50807\n",
      "   6720/  15000: loss=1.0664, valloss=1.5207, Tavg=0.50841\n",
      "   6730/  15000: loss=1.0977, valloss=1.5158, Tavg=0.50875\n",
      "   6740/  15000: loss=1.1160, valloss=1.5233, Tavg=0.50910\n",
      "   6750/  15000: loss=1.0931, valloss=1.4928, Tavg=0.50944\n",
      "   6760/  15000: loss=1.0845, valloss=1.5077, Tavg=0.50977\n",
      "   6770/  15000: loss=1.1049, valloss=1.5172, Tavg=0.51011\n",
      "   6780/  15000: loss=1.0901, valloss=1.5341, Tavg=0.51044\n",
      "   6790/  15000: loss=1.0940, valloss=1.5258, Tavg=0.51077\n",
      "   6800/  15000: loss=1.0779, valloss=1.5227, Tavg=0.51110\n",
      "   6810/  15000: loss=1.0917, valloss=1.5159, Tavg=0.51143\n",
      "   6820/  15000: loss=1.0911, valloss=1.5080, Tavg=0.51176\n",
      "   6830/  15000: loss=1.0854, valloss=1.5233, Tavg=0.51209\n",
      "   6840/  15000: loss=1.0677, valloss=1.5253, Tavg=0.51242\n",
      "   6850/  15000: loss=1.0989, valloss=1.5227, Tavg=0.51275\n",
      "   6860/  15000: loss=1.0798, valloss=1.5244, Tavg=0.51309\n",
      "   6870/  15000: loss=1.0941, valloss=1.5386, Tavg=0.51341\n",
      "   6880/  15000: loss=1.0943, valloss=1.5405, Tavg=0.51374\n",
      "   6890/  15000: loss=1.0968, valloss=1.5194, Tavg=0.51406\n",
      "   6900/  15000: loss=1.0882, valloss=1.5297, Tavg=0.51439\n",
      "   6910/  15000: loss=1.0738, valloss=1.5067, Tavg=0.51471\n",
      "   6920/  15000: loss=1.0740, valloss=1.5252, Tavg=0.51503\n",
      "   6930/  15000: loss=1.0831, valloss=1.5319, Tavg=0.51535\n",
      "   6940/  15000: loss=1.0763, valloss=1.5244, Tavg=0.51567\n",
      "   6950/  15000: loss=1.0787, valloss=1.5182, Tavg=0.51599\n",
      "   6960/  15000: loss=1.1151, valloss=1.5243, Tavg=0.51631\n",
      "   6970/  15000: loss=1.0919, valloss=1.5328, Tavg=0.51663\n",
      "   6980/  15000: loss=1.0595, valloss=1.5310, Tavg=0.51694\n",
      "   6990/  15000: loss=1.0607, valloss=1.5279, Tavg=0.51725\n",
      "   7000/  15000: loss=1.0790, valloss=1.5116, Tavg=0.51757\n",
      "   7010/  15000: loss=1.0770, valloss=1.5371, Tavg=0.51789\n",
      "   7020/  15000: loss=1.0903, valloss=1.5204, Tavg=0.51820\n",
      "   7030/  15000: loss=1.0651, valloss=1.5204, Tavg=0.51852\n",
      "   7040/  15000: loss=1.0816, valloss=1.5312, Tavg=0.51884\n",
      "   7050/  15000: loss=1.0735, valloss=1.5311, Tavg=0.51915\n",
      "   7060/  15000: loss=1.0750, valloss=1.5306, Tavg=0.51946\n",
      "   7070/  15000: loss=1.0459, valloss=1.5290, Tavg=0.51977\n",
      "   7080/  15000: loss=1.0702, valloss=1.5357, Tavg=0.52008\n",
      "   7090/  15000: loss=1.0710, valloss=1.5285, Tavg=0.52040\n",
      "   7100/  15000: loss=1.0866, valloss=1.5382, Tavg=0.52071\n",
      "   7110/  15000: loss=1.0769, valloss=1.5411, Tavg=0.52102\n",
      "   7120/  15000: loss=1.0704, valloss=1.5392, Tavg=0.52133\n",
      "   7130/  15000: loss=1.0774, valloss=1.5196, Tavg=0.52163\n",
      "   7140/  15000: loss=1.0636, valloss=1.5410, Tavg=0.52194\n",
      "   7150/  15000: loss=1.0744, valloss=1.5279, Tavg=0.52224\n",
      "   7160/  15000: loss=1.0398, valloss=1.5366, Tavg=0.52254\n",
      "   7170/  15000: loss=1.0762, valloss=1.5044, Tavg=0.52284\n",
      "   7180/  15000: loss=1.0585, valloss=1.5174, Tavg=0.52314\n",
      "   7190/  15000: loss=1.0860, valloss=1.5436, Tavg=0.52343\n",
      "   7200/  15000: loss=1.0594, valloss=1.5179, Tavg=0.52373\n",
      "   7210/  15000: loss=1.0518, valloss=1.5305, Tavg=0.52404\n",
      "   7220/  15000: loss=1.0482, valloss=1.5332, Tavg=0.52433\n",
      "   7230/  15000: loss=1.0611, valloss=1.5335, Tavg=0.52463\n",
      "   7240/  15000: loss=1.0660, valloss=1.5443, Tavg=0.52492\n",
      "   7250/  15000: loss=1.0509, valloss=1.5374, Tavg=0.52520\n",
      "   7260/  15000: loss=1.0712, valloss=1.5428, Tavg=0.52550\n",
      "   7270/  15000: loss=1.0610, valloss=1.5433, Tavg=0.52578\n",
      "   7280/  15000: loss=1.0370, valloss=1.5467, Tavg=0.52607\n",
      "   7290/  15000: loss=1.0698, valloss=1.5514, Tavg=0.52636\n",
      "   7300/  15000: loss=1.0672, valloss=1.5500, Tavg=0.52666\n",
      "   7310/  15000: loss=1.0720, valloss=1.5325, Tavg=0.52695\n",
      "   7320/  15000: loss=1.0198, valloss=1.5410, Tavg=0.52723\n",
      "   7330/  15000: loss=1.0896, valloss=1.5343, Tavg=0.52751\n",
      "   7340/  15000: loss=1.0504, valloss=1.5329, Tavg=0.52780\n",
      "   7350/  15000: loss=1.0704, valloss=1.5321, Tavg=0.52809\n",
      "   7360/  15000: loss=0.9993, valloss=1.5432, Tavg=0.52836\n",
      "   7370/  15000: loss=1.0172, valloss=1.5510, Tavg=0.52864\n",
      "   7380/  15000: loss=1.0726, valloss=1.5241, Tavg=0.52893\n",
      "   7390/  15000: loss=1.0473, valloss=1.5389, Tavg=0.52921\n",
      "   7400/  15000: loss=1.0342, valloss=1.5634, Tavg=0.52949\n",
      "   7410/  15000: loss=1.0400, valloss=1.5454, Tavg=0.52977\n",
      "   7420/  15000: loss=1.0253, valloss=1.5404, Tavg=0.53006\n",
      "   7430/  15000: loss=1.0534, valloss=1.5311, Tavg=0.53033\n",
      "   7440/  15000: loss=1.0543, valloss=1.5360, Tavg=0.53060\n",
      "   7450/  15000: loss=1.0445, valloss=1.5252, Tavg=0.53088\n",
      "   7460/  15000: loss=1.0766, valloss=1.5569, Tavg=0.53115\n",
      "   7470/  15000: loss=1.0650, valloss=1.5350, Tavg=0.53143\n",
      "   7480/  15000: loss=1.0680, valloss=1.5419, Tavg=0.53170\n",
      "   7490/  15000: loss=1.0622, valloss=1.5459, Tavg=0.53197\n",
      "   7500/  15000: loss=1.0528, valloss=1.5563, Tavg=0.53224\n",
      "   7510/  15000: loss=1.0506, valloss=1.5407, Tavg=0.53251\n",
      "   7520/  15000: loss=1.0652, valloss=1.5287, Tavg=0.53279\n",
      "   7530/  15000: loss=1.0450, valloss=1.5478, Tavg=0.53306\n",
      "   7540/  15000: loss=1.0596, valloss=1.5451, Tavg=0.53332\n",
      "   7550/  15000: loss=1.0416, valloss=1.5562, Tavg=0.53359\n",
      "   7560/  15000: loss=1.0538, valloss=1.5436, Tavg=0.53385\n",
      "   7570/  15000: loss=1.0598, valloss=1.5404, Tavg=0.53412\n",
      "   7580/  15000: loss=1.0470, valloss=1.5649, Tavg=0.53439\n",
      "   7590/  15000: loss=1.0425, valloss=1.5395, Tavg=0.53466\n",
      "   7600/  15000: loss=1.0337, valloss=1.5405, Tavg=0.53493\n",
      "   7610/  15000: loss=1.0638, valloss=1.5339, Tavg=0.53519\n",
      "   7620/  15000: loss=1.0421, valloss=1.5423, Tavg=0.53545\n",
      "   7630/  15000: loss=1.0725, valloss=1.5360, Tavg=0.53572\n",
      "   7640/  15000: loss=1.0419, valloss=1.5568, Tavg=0.53598\n",
      "   7650/  15000: loss=1.0508, valloss=1.5558, Tavg=0.53622\n",
      "   7660/  15000: loss=1.0333, valloss=1.5533, Tavg=0.53649\n",
      "   7670/  15000: loss=1.0776, valloss=1.5404, Tavg=0.53676\n",
      "   7680/  15000: loss=1.0251, valloss=1.5511, Tavg=0.53701\n",
      "   7690/  15000: loss=1.0577, valloss=1.5598, Tavg=0.53728\n",
      "   7700/  15000: loss=1.0237, valloss=1.5526, Tavg=0.53753\n",
      "   7710/  15000: loss=1.0194, valloss=1.5549, Tavg=0.53779\n",
      "   7720/  15000: loss=1.0276, valloss=1.5548, Tavg=0.53805\n",
      "   7730/  15000: loss=1.0617, valloss=1.5496, Tavg=0.53830\n",
      "   7740/  15000: loss=1.0416, valloss=1.5588, Tavg=0.53856\n",
      "   7750/  15000: loss=1.0510, valloss=1.5519, Tavg=0.53882\n",
      "   7760/  15000: loss=1.0507, valloss=1.5603, Tavg=0.53907\n",
      "   7770/  15000: loss=1.0137, valloss=1.5501, Tavg=0.53932\n",
      "   7780/  15000: loss=1.0203, valloss=1.5526, Tavg=0.53958\n",
      "   7790/  15000: loss=1.0177, valloss=1.5462, Tavg=0.53983\n",
      "   7800/  15000: loss=1.0201, valloss=1.5641, Tavg=0.54008\n",
      "   7810/  15000: loss=1.0385, valloss=1.5507, Tavg=0.54034\n",
      "   7820/  15000: loss=1.0726, valloss=1.5397, Tavg=0.54059\n",
      "   7830/  15000: loss=1.0399, valloss=1.5608, Tavg=0.54085\n",
      "   7840/  15000: loss=1.0401, valloss=1.5649, Tavg=0.54110\n",
      "   7850/  15000: loss=1.0458, valloss=1.5528, Tavg=0.54135\n",
      "   7860/  15000: loss=1.0281, valloss=1.5569, Tavg=0.54159\n",
      "   7870/  15000: loss=1.0238, valloss=1.5678, Tavg=0.54183\n",
      "   7880/  15000: loss=1.0488, valloss=1.5534, Tavg=0.54208\n",
      "   7890/  15000: loss=1.0316, valloss=1.5573, Tavg=0.54232\n",
      "   7900/  15000: loss=1.0174, valloss=1.5702, Tavg=0.54256\n",
      "   7910/  15000: loss=1.0300, valloss=1.5540, Tavg=0.54281\n",
      "   7920/  15000: loss=1.0278, valloss=1.5760, Tavg=0.54305\n",
      "   7930/  15000: loss=1.0654, valloss=1.5606, Tavg=0.54329\n",
      "   7940/  15000: loss=1.0208, valloss=1.5620, Tavg=0.54353\n",
      "   7950/  15000: loss=1.0491, valloss=1.5524, Tavg=0.54377\n",
      "   7960/  15000: loss=1.0341, valloss=1.5625, Tavg=0.54400\n",
      "   7970/  15000: loss=1.0131, valloss=1.5812, Tavg=0.54423\n",
      "   7980/  15000: loss=1.0417, valloss=1.5516, Tavg=0.54447\n",
      "   7990/  15000: loss=1.0332, valloss=1.5693, Tavg=0.54470\n",
      "   8000/  15000: loss=1.0087, valloss=1.5546, Tavg=0.54494\n",
      "   8010/  15000: loss=1.0362, valloss=1.5529, Tavg=0.54517\n",
      "   8020/  15000: loss=1.0544, valloss=1.5653, Tavg=0.54540\n",
      "   8030/  15000: loss=1.0294, valloss=1.5728, Tavg=0.54564\n",
      "   8040/  15000: loss=1.0323, valloss=1.5509, Tavg=0.54587\n",
      "   8050/  15000: loss=1.0481, valloss=1.5655, Tavg=0.54611\n",
      "   8060/  15000: loss=1.0580, valloss=1.5683, Tavg=0.54634\n",
      "   8070/  15000: loss=1.0267, valloss=1.5715, Tavg=0.54657\n",
      "   8080/  15000: loss=1.0161, valloss=1.5617, Tavg=0.54680\n",
      "   8090/  15000: loss=1.0563, valloss=1.5519, Tavg=0.54704\n",
      "   8100/  15000: loss=1.0281, valloss=1.5498, Tavg=0.54727\n",
      "   8110/  15000: loss=1.0414, valloss=1.5579, Tavg=0.54750\n",
      "   8120/  15000: loss=1.0083, valloss=1.5625, Tavg=0.54774\n",
      "   8130/  15000: loss=1.0266, valloss=1.5751, Tavg=0.54797\n",
      "   8140/  15000: loss=1.0020, valloss=1.5685, Tavg=0.54820\n",
      "   8150/  15000: loss=1.0176, valloss=1.5888, Tavg=0.54844\n",
      "   8160/  15000: loss=1.0035, valloss=1.5713, Tavg=0.54867\n",
      "   8170/  15000: loss=1.0192, valloss=1.5704, Tavg=0.54890\n",
      "   8180/  15000: loss=1.0129, valloss=1.5703, Tavg=0.54912\n",
      "   8190/  15000: loss=1.0219, valloss=1.5549, Tavg=0.54935\n",
      "   8200/  15000: loss=1.0365, valloss=1.5721, Tavg=0.54958\n",
      "   8210/  15000: loss=1.0215, valloss=1.5769, Tavg=0.54980\n",
      "   8220/  15000: loss=0.9976, valloss=1.5818, Tavg=0.55003\n",
      "   8230/  15000: loss=0.9833, valloss=1.5757, Tavg=0.55026\n",
      "   8240/  15000: loss=1.0390, valloss=1.5397, Tavg=0.55049\n",
      "   8250/  15000: loss=1.0372, valloss=1.5693, Tavg=0.55072\n",
      "   8260/  15000: loss=0.9876, valloss=1.5708, Tavg=0.55094\n",
      "   8270/  15000: loss=1.0412, valloss=1.5832, Tavg=0.55117\n",
      "   8280/  15000: loss=1.0006, valloss=1.5704, Tavg=0.55139\n",
      "   8290/  15000: loss=1.0177, valloss=1.5722, Tavg=0.55161\n",
      "   8300/  15000: loss=1.0249, valloss=1.5787, Tavg=0.55183\n",
      "   8310/  15000: loss=0.9794, valloss=1.5876, Tavg=0.55205\n",
      "   8320/  15000: loss=0.9900, valloss=1.5962, Tavg=0.55227\n",
      "   8330/  15000: loss=1.0101, valloss=1.5683, Tavg=0.55249\n",
      "   8340/  15000: loss=1.0299, valloss=1.5649, Tavg=0.55271\n",
      "   8350/  15000: loss=1.0223, valloss=1.5722, Tavg=0.55294\n",
      "   8360/  15000: loss=1.0189, valloss=1.5654, Tavg=0.55315\n",
      "   8370/  15000: loss=1.0021, valloss=1.5807, Tavg=0.55337\n",
      "   8380/  15000: loss=1.0346, valloss=1.5702, Tavg=0.55359\n",
      "   8390/  15000: loss=1.0133, valloss=1.5981, Tavg=0.55380\n",
      "   8400/  15000: loss=1.0110, valloss=1.5755, Tavg=0.55402\n",
      "   8410/  15000: loss=1.0245, valloss=1.5753, Tavg=0.55424\n",
      "   8420/  15000: loss=1.0251, valloss=1.5993, Tavg=0.55446\n",
      "   8430/  15000: loss=1.0270, valloss=1.5864, Tavg=0.55467\n",
      "   8440/  15000: loss=1.0070, valloss=1.5817, Tavg=0.55489\n",
      "   8450/  15000: loss=1.0179, valloss=1.6039, Tavg=0.55511\n",
      "   8460/  15000: loss=0.9993, valloss=1.5709, Tavg=0.55532\n",
      "   8470/  15000: loss=1.0083, valloss=1.5877, Tavg=0.55553\n",
      "   8480/  15000: loss=0.9893, valloss=1.5731, Tavg=0.55574\n",
      "   8490/  15000: loss=1.0290, valloss=1.5799, Tavg=0.55595\n",
      "   8500/  15000: loss=0.9956, valloss=1.5740, Tavg=0.55616\n",
      "   8510/  15000: loss=1.0074, valloss=1.5747, Tavg=0.55638\n",
      "   8520/  15000: loss=1.0084, valloss=1.5930, Tavg=0.55659\n",
      "   8530/  15000: loss=1.0065, valloss=1.5828, Tavg=0.55680\n",
      "   8540/  15000: loss=0.9923, valloss=1.5908, Tavg=0.55701\n",
      "   8550/  15000: loss=1.0025, valloss=1.5710, Tavg=0.55722\n",
      "   8560/  15000: loss=1.0186, valloss=1.5708, Tavg=0.55744\n",
      "   8570/  15000: loss=1.0117, valloss=1.5877, Tavg=0.55764\n",
      "   8580/  15000: loss=1.0032, valloss=1.5938, Tavg=0.55785\n",
      "   8590/  15000: loss=1.0259, valloss=1.5918, Tavg=0.55806\n",
      "   8600/  15000: loss=0.9924, valloss=1.5899, Tavg=0.55826\n",
      "   8610/  15000: loss=0.9869, valloss=1.5872, Tavg=0.55847\n",
      "   8620/  15000: loss=0.9947, valloss=1.6041, Tavg=0.55868\n",
      "   8630/  15000: loss=0.9980, valloss=1.5918, Tavg=0.55888\n",
      "   8640/  15000: loss=0.9623, valloss=1.5860, Tavg=0.55909\n",
      "   8650/  15000: loss=0.9932, valloss=1.5669, Tavg=0.55930\n",
      "   8660/  15000: loss=0.9971, valloss=1.5997, Tavg=0.55950\n",
      "   8670/  15000: loss=0.9818, valloss=1.5833, Tavg=0.55970\n",
      "   8680/  15000: loss=0.9996, valloss=1.5995, Tavg=0.55990\n",
      "   8690/  15000: loss=0.9881, valloss=1.6068, Tavg=0.56010\n",
      "   8700/  15000: loss=0.9867, valloss=1.5987, Tavg=0.56030\n",
      "   8710/  15000: loss=0.9800, valloss=1.6023, Tavg=0.56050\n",
      "   8720/  15000: loss=1.0059, valloss=1.5884, Tavg=0.56070\n",
      "   8730/  15000: loss=0.9796, valloss=1.6095, Tavg=0.56091\n",
      "   8740/  15000: loss=1.0051, valloss=1.5884, Tavg=0.56111\n",
      "   8750/  15000: loss=0.9830, valloss=1.5911, Tavg=0.56131\n",
      "   8760/  15000: loss=1.0132, valloss=1.5947, Tavg=0.56151\n",
      "   8770/  15000: loss=0.9914, valloss=1.5913, Tavg=0.56170\n",
      "   8780/  15000: loss=1.0219, valloss=1.6001, Tavg=0.56190\n",
      "   8790/  15000: loss=1.0043, valloss=1.5939, Tavg=0.56210\n",
      "   8800/  15000: loss=0.9872, valloss=1.5784, Tavg=0.56230\n",
      "   8810/  15000: loss=1.0068, valloss=1.5959, Tavg=0.56250\n",
      "   8820/  15000: loss=0.9844, valloss=1.5970, Tavg=0.56270\n",
      "   8830/  15000: loss=1.0098, valloss=1.5760, Tavg=0.56289\n",
      "   8840/  15000: loss=0.9867, valloss=1.5826, Tavg=0.56309\n",
      "   8850/  15000: loss=0.9888, valloss=1.6007, Tavg=0.56329\n",
      "   8860/  15000: loss=0.9642, valloss=1.6010, Tavg=0.56348\n",
      "   8870/  15000: loss=0.9698, valloss=1.5930, Tavg=0.56367\n",
      "   8880/  15000: loss=0.9716, valloss=1.5922, Tavg=0.56386\n",
      "   8890/  15000: loss=0.9906, valloss=1.6025, Tavg=0.56406\n",
      "   8900/  15000: loss=0.9913, valloss=1.6050, Tavg=0.56425\n",
      "   8910/  15000: loss=1.0039, valloss=1.5993, Tavg=0.56444\n",
      "   8920/  15000: loss=1.0012, valloss=1.5917, Tavg=0.56464\n",
      "   8930/  15000: loss=1.0049, valloss=1.6152, Tavg=0.56484\n",
      "   8940/  15000: loss=0.9883, valloss=1.6179, Tavg=0.56503\n",
      "   8950/  15000: loss=0.9787, valloss=1.5953, Tavg=0.56522\n",
      "   8960/  15000: loss=0.9933, valloss=1.5875, Tavg=0.56541\n",
      "   8970/  15000: loss=1.0006, valloss=1.6121, Tavg=0.56559\n",
      "   8980/  15000: loss=0.9969, valloss=1.5982, Tavg=0.56579\n",
      "   8990/  15000: loss=1.0069, valloss=1.6033, Tavg=0.56598\n",
      "   9000/  15000: loss=0.9895, valloss=1.5965, Tavg=0.56617\n",
      "   9010/  15000: loss=0.9784, valloss=1.5957, Tavg=0.56636\n",
      "   9020/  15000: loss=0.9867, valloss=1.5917, Tavg=0.56655\n",
      "   9030/  15000: loss=0.9653, valloss=1.6032, Tavg=0.56674\n",
      "   9040/  15000: loss=0.9555, valloss=1.6115, Tavg=0.56692\n",
      "   9050/  15000: loss=0.9777, valloss=1.6006, Tavg=0.56710\n",
      "   9060/  15000: loss=0.9858, valloss=1.5882, Tavg=0.56729\n",
      "   9070/  15000: loss=0.9862, valloss=1.5995, Tavg=0.56747\n",
      "   9080/  15000: loss=0.9715, valloss=1.5986, Tavg=0.56766\n",
      "   9090/  15000: loss=0.9805, valloss=1.6027, Tavg=0.56785\n",
      "   9100/  15000: loss=0.9858, valloss=1.6032, Tavg=0.56803\n",
      "   9110/  15000: loss=1.0027, valloss=1.6202, Tavg=0.56822\n",
      "   9120/  15000: loss=0.9723, valloss=1.6064, Tavg=0.56840\n",
      "   9130/  15000: loss=0.9867, valloss=1.6340, Tavg=0.56859\n",
      "   9140/  15000: loss=0.9904, valloss=1.5933, Tavg=0.56877\n",
      "   9150/  15000: loss=0.9830, valloss=1.6134, Tavg=0.56895\n",
      "   9160/  15000: loss=0.9810, valloss=1.5963, Tavg=0.56913\n",
      "   9170/  15000: loss=0.9741, valloss=1.6276, Tavg=0.56931\n",
      "   9180/  15000: loss=0.9800, valloss=1.6201, Tavg=0.56950\n",
      "   9190/  15000: loss=0.9832, valloss=1.6059, Tavg=0.56968\n",
      "   9200/  15000: loss=0.9512, valloss=1.6294, Tavg=0.56986\n",
      "   9210/  15000: loss=0.9689, valloss=1.6268, Tavg=0.57005\n",
      "   9220/  15000: loss=0.9665, valloss=1.5950, Tavg=0.57023\n",
      "   9230/  15000: loss=0.9543, valloss=1.6141, Tavg=0.57041\n",
      "   9240/  15000: loss=0.9925, valloss=1.6243, Tavg=0.57058\n",
      "   9250/  15000: loss=0.9803, valloss=1.5883, Tavg=0.57075\n",
      "   9260/  15000: loss=0.9803, valloss=1.6031, Tavg=0.57093\n",
      "   9270/  15000: loss=0.9651, valloss=1.5976, Tavg=0.57111\n",
      "   9280/  15000: loss=0.9670, valloss=1.6096, Tavg=0.57130\n",
      "   9290/  15000: loss=0.9597, valloss=1.6296, Tavg=0.57148\n",
      "   9300/  15000: loss=0.9522, valloss=1.6237, Tavg=0.57166\n",
      "   9310/  15000: loss=0.9702, valloss=1.6167, Tavg=0.57184\n",
      "   9320/  15000: loss=0.9735, valloss=1.6098, Tavg=0.57201\n",
      "   9330/  15000: loss=0.9716, valloss=1.6043, Tavg=0.57218\n",
      "   9340/  15000: loss=0.9409, valloss=1.6151, Tavg=0.57235\n",
      "   9350/  15000: loss=0.9687, valloss=1.6034, Tavg=0.57252\n",
      "   9360/  15000: loss=0.9718, valloss=1.5984, Tavg=0.57270\n",
      "   9370/  15000: loss=0.9574, valloss=1.6093, Tavg=0.57288\n",
      "   9380/  15000: loss=0.9630, valloss=1.6068, Tavg=0.57306\n",
      "   9390/  15000: loss=0.9609, valloss=1.6271, Tavg=0.57323\n",
      "   9400/  15000: loss=0.9480, valloss=1.6018, Tavg=0.57340\n",
      "   9410/  15000: loss=0.9641, valloss=1.6278, Tavg=0.57357\n",
      "   9420/  15000: loss=0.9526, valloss=1.6395, Tavg=0.57374\n",
      "   9430/  15000: loss=0.9714, valloss=1.6288, Tavg=0.57391\n",
      "   9440/  15000: loss=0.9506, valloss=1.6292, Tavg=0.57408\n",
      "   9450/  15000: loss=0.9788, valloss=1.6186, Tavg=0.57425\n",
      "   9460/  15000: loss=0.9717, valloss=1.6194, Tavg=0.57443\n",
      "   9470/  15000: loss=0.9659, valloss=1.6278, Tavg=0.57460\n",
      "   9480/  15000: loss=0.9690, valloss=1.6300, Tavg=0.57476\n",
      "   9490/  15000: loss=0.9554, valloss=1.6139, Tavg=0.57493\n",
      "   9500/  15000: loss=0.9541, valloss=1.6237, Tavg=0.57510\n",
      "   9510/  15000: loss=0.9436, valloss=1.6320, Tavg=0.57528\n",
      "   9520/  15000: loss=0.9715, valloss=1.6243, Tavg=0.57544\n",
      "   9530/  15000: loss=0.9345, valloss=1.6031, Tavg=0.57561\n",
      "   9540/  15000: loss=0.9635, valloss=1.6278, Tavg=0.57577\n",
      "   9550/  15000: loss=0.9606, valloss=1.6282, Tavg=0.57594\n",
      "   9560/  15000: loss=0.9722, valloss=1.6124, Tavg=0.57610\n",
      "   9570/  15000: loss=0.9600, valloss=1.6197, Tavg=0.57626\n",
      "   9580/  15000: loss=0.9595, valloss=1.6195, Tavg=0.57643\n",
      "   9590/  15000: loss=0.9287, valloss=1.6179, Tavg=0.57659\n",
      "   9600/  15000: loss=0.9576, valloss=1.6049, Tavg=0.57675\n",
      "   9610/  15000: loss=0.9431, valloss=1.6371, Tavg=0.57691\n",
      "   9620/  15000: loss=0.9615, valloss=1.6308, Tavg=0.57708\n",
      "   9630/  15000: loss=0.9523, valloss=1.6521, Tavg=0.57724\n",
      "   9640/  15000: loss=0.9869, valloss=1.6453, Tavg=0.57741\n",
      "   9650/  15000: loss=0.9650, valloss=1.6383, Tavg=0.57757\n",
      "   9660/  15000: loss=0.9520, valloss=1.6291, Tavg=0.57773\n",
      "   9670/  15000: loss=0.9420, valloss=1.6393, Tavg=0.57789\n",
      "   9680/  15000: loss=0.9662, valloss=1.6237, Tavg=0.57805\n",
      "   9690/  15000: loss=0.9473, valloss=1.6456, Tavg=0.57822\n",
      "   9700/  15000: loss=0.9599, valloss=1.6201, Tavg=0.57838\n",
      "   9710/  15000: loss=0.9402, valloss=1.6305, Tavg=0.57854\n",
      "   9720/  15000: loss=0.9549, valloss=1.6265, Tavg=0.57870\n",
      "   9730/  15000: loss=0.9192, valloss=1.6500, Tavg=0.57886\n",
      "   9740/  15000: loss=0.9633, valloss=1.6387, Tavg=0.57902\n",
      "   9750/  15000: loss=0.9351, valloss=1.6296, Tavg=0.57918\n",
      "   9760/  15000: loss=0.9337, valloss=1.6417, Tavg=0.57934\n",
      "   9770/  15000: loss=0.9719, valloss=1.6283, Tavg=0.57951\n",
      "   9780/  15000: loss=0.9490, valloss=1.6302, Tavg=0.57966\n",
      "   9790/  15000: loss=0.9519, valloss=1.6206, Tavg=0.57983\n",
      "   9800/  15000: loss=0.9336, valloss=1.6234, Tavg=0.57998\n",
      "   9810/  15000: loss=0.9636, valloss=1.6356, Tavg=0.58013\n",
      "   9820/  15000: loss=0.9718, valloss=1.6407, Tavg=0.58029\n",
      "   9830/  15000: loss=0.9433, valloss=1.6295, Tavg=0.58046\n",
      "   9840/  15000: loss=0.9390, valloss=1.6322, Tavg=0.58062\n",
      "   9850/  15000: loss=0.9589, valloss=1.6246, Tavg=0.58078\n",
      "   9860/  15000: loss=0.9197, valloss=1.6329, Tavg=0.58094\n",
      "   9870/  15000: loss=0.9775, valloss=1.6286, Tavg=0.58110\n",
      "   9880/  15000: loss=0.9763, valloss=1.6636, Tavg=0.58126\n",
      "   9890/  15000: loss=0.9309, valloss=1.6400, Tavg=0.58141\n",
      "   9900/  15000: loss=0.9661, valloss=1.6427, Tavg=0.58156\n",
      "   9910/  15000: loss=0.9365, valloss=1.6540, Tavg=0.58171\n",
      "   9920/  15000: loss=0.9650, valloss=1.6491, Tavg=0.58187\n",
      "   9930/  15000: loss=0.9437, valloss=1.6239, Tavg=0.58202\n",
      "   9940/  15000: loss=0.9651, valloss=1.6390, Tavg=0.58218\n",
      "   9950/  15000: loss=0.9209, valloss=1.6631, Tavg=0.58234\n",
      "   9960/  15000: loss=0.9294, valloss=1.6633, Tavg=0.58250\n",
      "   9970/  15000: loss=0.9316, valloss=1.6236, Tavg=0.58266\n",
      "   9980/  15000: loss=0.9276, valloss=1.6548, Tavg=0.58281\n",
      "   9990/  15000: loss=0.9396, valloss=1.6336, Tavg=0.58296\n",
      "  10000/  15000: loss=0.9344, valloss=1.6386, Tavg=0.58311\n",
      "  10010/  15000: loss=0.9634, valloss=1.6446, Tavg=0.58326\n",
      "  10020/  15000: loss=0.9409, valloss=1.6573, Tavg=0.58342\n",
      "  10030/  15000: loss=0.9648, valloss=1.6442, Tavg=0.58357\n",
      "  10040/  15000: loss=0.9420, valloss=1.6533, Tavg=0.58372\n",
      "  10050/  15000: loss=0.9444, valloss=1.6454, Tavg=0.58387\n",
      "  10060/  15000: loss=0.9098, valloss=1.6437, Tavg=0.58402\n",
      "  10070/  15000: loss=0.9281, valloss=1.6471, Tavg=0.58417\n",
      "  10080/  15000: loss=0.9392, valloss=1.6570, Tavg=0.58432\n",
      "  10090/  15000: loss=0.9177, valloss=1.6508, Tavg=0.58447\n",
      "  10100/  15000: loss=0.9321, valloss=1.6624, Tavg=0.58461\n",
      "  10110/  15000: loss=0.9206, valloss=1.6585, Tavg=0.58476\n",
      "  10120/  15000: loss=0.9260, valloss=1.6446, Tavg=0.58492\n",
      "  10130/  15000: loss=0.9172, valloss=1.6587, Tavg=0.58507\n",
      "  10140/  15000: loss=0.9377, valloss=1.6452, Tavg=0.58522\n",
      "  10150/  15000: loss=0.9075, valloss=1.6513, Tavg=0.58537\n",
      "  10160/  15000: loss=0.9213, valloss=1.6667, Tavg=0.58552\n",
      "  10170/  15000: loss=0.9166, valloss=1.6445, Tavg=0.58567\n",
      "  10180/  15000: loss=0.9343, valloss=1.6541, Tavg=0.58581\n",
      "  10190/  15000: loss=0.9142, valloss=1.6547, Tavg=0.58595\n",
      "  10200/  15000: loss=0.9309, valloss=1.6639, Tavg=0.58610\n",
      "  10210/  15000: loss=0.9780, valloss=1.6829, Tavg=0.58624\n",
      "  10220/  15000: loss=0.9099, valloss=1.6817, Tavg=0.58639\n",
      "  10230/  15000: loss=0.9155, valloss=1.6718, Tavg=0.58654\n",
      "  10240/  15000: loss=0.9241, valloss=1.6583, Tavg=0.58668\n",
      "  10250/  15000: loss=0.9055, valloss=1.6380, Tavg=0.58683\n",
      "  10260/  15000: loss=0.9368, valloss=1.6569, Tavg=0.58698\n",
      "  10270/  15000: loss=0.9327, valloss=1.6765, Tavg=0.58713\n",
      "  10280/  15000: loss=0.9501, valloss=1.6429, Tavg=0.58727\n",
      "  10290/  15000: loss=0.9392, valloss=1.6645, Tavg=0.58741\n",
      "  10300/  15000: loss=0.9393, valloss=1.6444, Tavg=0.58755\n",
      "  10310/  15000: loss=0.9193, valloss=1.6828, Tavg=0.58770\n",
      "  10320/  15000: loss=0.9161, valloss=1.6667, Tavg=0.58785\n",
      "  10330/  15000: loss=0.9152, valloss=1.6700, Tavg=0.58800\n",
      "  10340/  15000: loss=0.9302, valloss=1.6801, Tavg=0.58814\n",
      "  10350/  15000: loss=0.9045, valloss=1.6653, Tavg=0.58829\n",
      "  10360/  15000: loss=0.9166, valloss=1.6557, Tavg=0.58843\n",
      "  10370/  15000: loss=0.9351, valloss=1.6737, Tavg=0.58857\n",
      "  10380/  15000: loss=0.9269, valloss=1.6645, Tavg=0.58872\n",
      "  10390/  15000: loss=0.9052, valloss=1.6767, Tavg=0.58886\n",
      "  10400/  15000: loss=0.9170, valloss=1.6513, Tavg=0.58900\n",
      "  10410/  15000: loss=0.9342, valloss=1.6777, Tavg=0.58915\n",
      "  10420/  15000: loss=0.8951, valloss=1.6738, Tavg=0.58929\n",
      "  10430/  15000: loss=0.9042, valloss=1.6894, Tavg=0.58943\n",
      "  10440/  15000: loss=0.9189, valloss=1.6762, Tavg=0.58957\n",
      "  10450/  15000: loss=0.9414, valloss=1.6719, Tavg=0.58971\n",
      "  10460/  15000: loss=0.9213, valloss=1.6907, Tavg=0.58984\n",
      "  10470/  15000: loss=0.9074, valloss=1.6692, Tavg=0.58998\n",
      "  10480/  15000: loss=0.9185, valloss=1.6628, Tavg=0.59011\n",
      "  10490/  15000: loss=0.9225, valloss=1.6611, Tavg=0.59025\n",
      "  10500/  15000: loss=0.9230, valloss=1.6706, Tavg=0.59039\n",
      "  10510/  15000: loss=0.8919, valloss=1.6805, Tavg=0.59053\n",
      "  10520/  15000: loss=0.9323, valloss=1.6764, Tavg=0.59067\n",
      "  10530/  15000: loss=0.9418, valloss=1.6824, Tavg=0.59080\n",
      "  10540/  15000: loss=0.9087, valloss=1.6711, Tavg=0.59093\n",
      "  10550/  15000: loss=0.9332, valloss=1.6621, Tavg=0.59107\n",
      "  10560/  15000: loss=0.9340, valloss=1.6784, Tavg=0.59120\n",
      "  10570/  15000: loss=0.8851, valloss=1.6651, Tavg=0.59133\n",
      "  10580/  15000: loss=0.9166, valloss=1.6488, Tavg=0.59146\n",
      "  10590/  15000: loss=0.8971, valloss=1.6757, Tavg=0.59160\n",
      "  10600/  15000: loss=0.9408, valloss=1.6640, Tavg=0.59174\n",
      "  10610/  15000: loss=0.9596, valloss=1.6862, Tavg=0.59187\n",
      "  10620/  15000: loss=0.9196, valloss=1.6931, Tavg=0.59200\n",
      "  10630/  15000: loss=0.9165, valloss=1.6757, Tavg=0.59214\n",
      "  10640/  15000: loss=0.9208, valloss=1.6649, Tavg=0.59227\n",
      "  10650/  15000: loss=0.9122, valloss=1.6952, Tavg=0.59241\n",
      "  10660/  15000: loss=0.8899, valloss=1.6749, Tavg=0.59254\n",
      "  10670/  15000: loss=0.8943, valloss=1.6888, Tavg=0.59267\n",
      "  10680/  15000: loss=0.9221, valloss=1.6626, Tavg=0.59280\n",
      "  10690/  15000: loss=0.9319, valloss=1.6948, Tavg=0.59294\n",
      "  10700/  15000: loss=0.9217, valloss=1.6592, Tavg=0.59308\n",
      "  10710/  15000: loss=0.9220, valloss=1.6875, Tavg=0.59321\n",
      "  10720/  15000: loss=0.8973, valloss=1.6916, Tavg=0.59334\n",
      "  10730/  15000: loss=0.9012, valloss=1.6698, Tavg=0.59347\n",
      "  10740/  15000: loss=0.9142, valloss=1.6544, Tavg=0.59360\n",
      "  10750/  15000: loss=0.8946, valloss=1.6901, Tavg=0.59373\n",
      "  10760/  15000: loss=0.9124, valloss=1.6790, Tavg=0.59385\n",
      "  10770/  15000: loss=0.9054, valloss=1.6958, Tavg=0.59398\n",
      "  10780/  15000: loss=0.9228, valloss=1.6745, Tavg=0.59411\n",
      "  10790/  15000: loss=0.9155, valloss=1.6914, Tavg=0.59424\n",
      "  10800/  15000: loss=0.9416, valloss=1.6855, Tavg=0.59437\n",
      "  10810/  15000: loss=0.9049, valloss=1.7016, Tavg=0.59450\n",
      "  10820/  15000: loss=0.8843, valloss=1.6798, Tavg=0.59462\n",
      "  10830/  15000: loss=0.9154, valloss=1.6894, Tavg=0.59475\n",
      "  10840/  15000: loss=0.9010, valloss=1.6947, Tavg=0.59488\n",
      "  10850/  15000: loss=0.9139, valloss=1.7069, Tavg=0.59501\n",
      "  10860/  15000: loss=0.9101, valloss=1.6855, Tavg=0.59514\n",
      "  10870/  15000: loss=0.9097, valloss=1.6980, Tavg=0.59527\n",
      "  10880/  15000: loss=0.8783, valloss=1.6896, Tavg=0.59540\n",
      "  10890/  15000: loss=0.8941, valloss=1.6972, Tavg=0.59553\n",
      "  10900/  15000: loss=0.9208, valloss=1.6739, Tavg=0.59565\n",
      "  10910/  15000: loss=0.9152, valloss=1.6909, Tavg=0.59578\n",
      "  10920/  15000: loss=0.8991, valloss=1.7114, Tavg=0.59591\n",
      "  10930/  15000: loss=0.8887, valloss=1.6714, Tavg=0.59604\n",
      "  10940/  15000: loss=0.8798, valloss=1.6879, Tavg=0.59617\n",
      "  10950/  15000: loss=0.9118, valloss=1.6913, Tavg=0.59629\n",
      "  10960/  15000: loss=0.9025, valloss=1.7068, Tavg=0.59642\n",
      "  10970/  15000: loss=0.9072, valloss=1.6885, Tavg=0.59654\n",
      "  10980/  15000: loss=0.8688, valloss=1.6915, Tavg=0.59667\n",
      "  10990/  15000: loss=0.9149, valloss=1.7139, Tavg=0.59680\n",
      "  11000/  15000: loss=0.9035, valloss=1.7037, Tavg=0.59693\n",
      "  11010/  15000: loss=0.8894, valloss=1.7058, Tavg=0.59706\n",
      "  11020/  15000: loss=0.8981, valloss=1.7087, Tavg=0.59718\n",
      "  11030/  15000: loss=0.8969, valloss=1.6911, Tavg=0.59731\n",
      "  11040/  15000: loss=0.8937, valloss=1.7058, Tavg=0.59743\n",
      "  11050/  15000: loss=0.8867, valloss=1.6869, Tavg=0.59756\n",
      "  11060/  15000: loss=0.9019, valloss=1.6843, Tavg=0.59768\n",
      "  11070/  15000: loss=0.8836, valloss=1.6962, Tavg=0.59781\n",
      "  11080/  15000: loss=0.8681, valloss=1.6815, Tavg=0.59793\n",
      "  11090/  15000: loss=0.8842, valloss=1.7171, Tavg=0.59806\n",
      "  11100/  15000: loss=0.8905, valloss=1.6965, Tavg=0.59819\n",
      "  11110/  15000: loss=0.8943, valloss=1.7055, Tavg=0.59831\n",
      "  11120/  15000: loss=0.9085, valloss=1.7112, Tavg=0.59844\n",
      "  11130/  15000: loss=0.9036, valloss=1.6896, Tavg=0.59856\n",
      "  11140/  15000: loss=0.8832, valloss=1.7035, Tavg=0.59868\n",
      "  11150/  15000: loss=0.8874, valloss=1.6831, Tavg=0.59880\n",
      "  11160/  15000: loss=0.8938, valloss=1.7135, Tavg=0.59892\n",
      "  11170/  15000: loss=0.9017, valloss=1.6911, Tavg=0.59905\n",
      "  11180/  15000: loss=0.8844, valloss=1.6863, Tavg=0.59917\n",
      "  11190/  15000: loss=0.8890, valloss=1.7141, Tavg=0.59929\n",
      "  11200/  15000: loss=0.8908, valloss=1.7015, Tavg=0.59941\n",
      "  11210/  15000: loss=0.8696, valloss=1.7000, Tavg=0.59954\n",
      "  11220/  15000: loss=0.8987, valloss=1.6859, Tavg=0.59965\n",
      "  11230/  15000: loss=0.9018, valloss=1.7110, Tavg=0.59977\n",
      "  11240/  15000: loss=0.8867, valloss=1.7056, Tavg=0.59989\n",
      "  11250/  15000: loss=0.8844, valloss=1.6921, Tavg=0.60001\n",
      "  11260/  15000: loss=0.8619, valloss=1.7088, Tavg=0.60013\n",
      "  11270/  15000: loss=0.8921, valloss=1.7089, Tavg=0.60025\n",
      "  11280/  15000: loss=0.8832, valloss=1.7246, Tavg=0.60037\n",
      "  11290/  15000: loss=0.8779, valloss=1.6845, Tavg=0.60049\n",
      "  11300/  15000: loss=0.8748, valloss=1.7204, Tavg=0.60061\n",
      "  11310/  15000: loss=0.9077, valloss=1.6928, Tavg=0.60073\n",
      "  11320/  15000: loss=0.8808, valloss=1.6975, Tavg=0.60085\n",
      "  11330/  15000: loss=0.9063, valloss=1.7157, Tavg=0.60097\n",
      "  11340/  15000: loss=0.8942, valloss=1.7120, Tavg=0.60108\n",
      "  11350/  15000: loss=0.8784, valloss=1.7099, Tavg=0.60121\n",
      "  11360/  15000: loss=0.8861, valloss=1.7227, Tavg=0.60133\n",
      "  11370/  15000: loss=0.8730, valloss=1.7097, Tavg=0.60145\n",
      "  11380/  15000: loss=0.8608, valloss=1.7086, Tavg=0.60157\n",
      "  11390/  15000: loss=0.8816, valloss=1.7039, Tavg=0.60169\n",
      "  11400/  15000: loss=0.8999, valloss=1.7223, Tavg=0.60181\n",
      "  11410/  15000: loss=0.8940, valloss=1.7372, Tavg=0.60192\n",
      "  11420/  15000: loss=0.8939, valloss=1.7267, Tavg=0.60204\n",
      "  11430/  15000: loss=0.8978, valloss=1.7273, Tavg=0.60215\n",
      "  11440/  15000: loss=0.8881, valloss=1.7173, Tavg=0.60226\n",
      "  11450/  15000: loss=0.8750, valloss=1.7261, Tavg=0.60238\n",
      "  11460/  15000: loss=0.8635, valloss=1.7182, Tavg=0.60250\n",
      "  11470/  15000: loss=0.8809, valloss=1.7333, Tavg=0.60262\n",
      "  11480/  15000: loss=0.8818, valloss=1.7281, Tavg=0.60274\n",
      "  11490/  15000: loss=0.8682, valloss=1.7262, Tavg=0.60285\n",
      "  11500/  15000: loss=0.8747, valloss=1.7146, Tavg=0.60297\n",
      "  11510/  15000: loss=0.8520, valloss=1.6996, Tavg=0.60308\n",
      "  11520/  15000: loss=0.8865, valloss=1.7135, Tavg=0.60319\n",
      "  11530/  15000: loss=0.9144, valloss=1.7235, Tavg=0.60331\n",
      "  11540/  15000: loss=0.8618, valloss=1.7064, Tavg=0.60342\n",
      "  11550/  15000: loss=0.8755, valloss=1.7257, Tavg=0.60353\n",
      "  11560/  15000: loss=0.8646, valloss=1.7154, Tavg=0.60364\n",
      "  11570/  15000: loss=0.8810, valloss=1.7313, Tavg=0.60376\n",
      "  11580/  15000: loss=0.8879, valloss=1.7252, Tavg=0.60387\n",
      "  11590/  15000: loss=0.8891, valloss=1.7030, Tavg=0.60399\n",
      "  11600/  15000: loss=0.8768, valloss=1.7183, Tavg=0.60410\n",
      "  11610/  15000: loss=0.8805, valloss=1.7347, Tavg=0.60421\n",
      "  11620/  15000: loss=0.8806, valloss=1.7191, Tavg=0.60432\n",
      "  11630/  15000: loss=0.8738, valloss=1.7343, Tavg=0.60444\n",
      "  11640/  15000: loss=0.8868, valloss=1.7262, Tavg=0.60456\n",
      "  11650/  15000: loss=0.8781, valloss=1.7438, Tavg=0.60467\n",
      "  11660/  15000: loss=0.8715, valloss=1.7252, Tavg=0.60478\n",
      "  11670/  15000: loss=0.8760, valloss=1.7345, Tavg=0.60489\n",
      "  11680/  15000: loss=0.8656, valloss=1.7212, Tavg=0.60500\n",
      "  11690/  15000: loss=0.8465, valloss=1.7150, Tavg=0.60511\n",
      "  11700/  15000: loss=0.8468, valloss=1.7351, Tavg=0.60522\n",
      "  11710/  15000: loss=0.8716, valloss=1.7410, Tavg=0.60534\n",
      "  11720/  15000: loss=0.8765, valloss=1.7227, Tavg=0.60546\n",
      "  11730/  15000: loss=0.8419, valloss=1.7279, Tavg=0.60557\n",
      "  11740/  15000: loss=0.8889, valloss=1.7333, Tavg=0.60568\n",
      "  11750/  15000: loss=0.8659, valloss=1.7210, Tavg=0.60580\n",
      "  11760/  15000: loss=0.8827, valloss=1.7473, Tavg=0.60591\n",
      "  11770/  15000: loss=0.8746, valloss=1.7285, Tavg=0.60602\n",
      "  11780/  15000: loss=0.8865, valloss=1.7394, Tavg=0.60613\n",
      "  11790/  15000: loss=0.8675, valloss=1.7318, Tavg=0.60624\n",
      "  11800/  15000: loss=0.8667, valloss=1.7385, Tavg=0.60636\n",
      "  11810/  15000: loss=0.8663, valloss=1.7474, Tavg=0.60646\n",
      "  11820/  15000: loss=0.8590, valloss=1.7289, Tavg=0.60657\n",
      "  11830/  15000: loss=0.8618, valloss=1.7537, Tavg=0.60668\n",
      "  11840/  15000: loss=0.8536, valloss=1.7285, Tavg=0.60679\n",
      "  11850/  15000: loss=0.8721, valloss=1.7342, Tavg=0.60690\n",
      "  11860/  15000: loss=0.8691, valloss=1.7415, Tavg=0.60701\n",
      "  11870/  15000: loss=0.8539, valloss=1.7515, Tavg=0.60712\n",
      "  11880/  15000: loss=0.8423, valloss=1.7312, Tavg=0.60723\n",
      "  11890/  15000: loss=0.8630, valloss=1.7420, Tavg=0.60734\n",
      "  11900/  15000: loss=0.8368, valloss=1.7221, Tavg=0.60745\n",
      "  11910/  15000: loss=0.8463, valloss=1.7332, Tavg=0.60755\n",
      "  11920/  15000: loss=0.8613, valloss=1.7397, Tavg=0.60767\n",
      "  11930/  15000: loss=0.8681, valloss=1.7401, Tavg=0.60778\n",
      "  11940/  15000: loss=0.8706, valloss=1.7499, Tavg=0.60789\n",
      "  11950/  15000: loss=0.8404, valloss=1.7308, Tavg=0.60800\n",
      "  11960/  15000: loss=0.8556, valloss=1.7450, Tavg=0.60811\n",
      "  11970/  15000: loss=0.8650, valloss=1.7597, Tavg=0.60822\n",
      "  11980/  15000: loss=0.8439, valloss=1.7455, Tavg=0.60833\n",
      "  11990/  15000: loss=0.8747, valloss=1.7171, Tavg=0.60843\n",
      "  12000/  15000: loss=0.8652, valloss=1.7430, Tavg=0.60853\n",
      "  12010/  15000: loss=0.8598, valloss=1.7251, Tavg=0.60864\n",
      "  12020/  15000: loss=0.8303, valloss=1.7385, Tavg=0.60875\n",
      "  12030/  15000: loss=0.8574, valloss=1.7300, Tavg=0.60886\n",
      "  12040/  15000: loss=0.8808, valloss=1.7366, Tavg=0.60897\n",
      "  12050/  15000: loss=0.8638, valloss=1.7400, Tavg=0.60907\n",
      "  12060/  15000: loss=0.8575, valloss=1.7419, Tavg=0.60918\n",
      "  12070/  15000: loss=0.8632, valloss=1.7580, Tavg=0.60928\n",
      "  12080/  15000: loss=0.8193, valloss=1.7653, Tavg=0.60939\n",
      "  12090/  15000: loss=0.8507, valloss=1.7583, Tavg=0.60950\n",
      "  12100/  15000: loss=0.8623, valloss=1.7577, Tavg=0.60960\n",
      "  12110/  15000: loss=0.8572, valloss=1.7442, Tavg=0.60971\n",
      "  12120/  15000: loss=0.8552, valloss=1.7295, Tavg=0.60981\n",
      "  12130/  15000: loss=0.8661, valloss=1.7516, Tavg=0.60991\n",
      "  12140/  15000: loss=0.8484, valloss=1.7400, Tavg=0.61001\n",
      "  12150/  15000: loss=0.8571, valloss=1.7503, Tavg=0.61011\n",
      "  12160/  15000: loss=0.8396, valloss=1.7620, Tavg=0.61021\n",
      "  12170/  15000: loss=0.8386, valloss=1.7537, Tavg=0.61031\n",
      "  12180/  15000: loss=0.8384, valloss=1.7543, Tavg=0.61041\n",
      "  12190/  15000: loss=0.8418, valloss=1.7438, Tavg=0.61051\n",
      "  12200/  15000: loss=0.8600, valloss=1.7463, Tavg=0.61061\n",
      "  12210/  15000: loss=0.8378, valloss=1.7446, Tavg=0.61071\n",
      "  12220/  15000: loss=0.8429, valloss=1.7547, Tavg=0.61081\n",
      "  12230/  15000: loss=0.8689, valloss=1.7705, Tavg=0.61091\n",
      "  12240/  15000: loss=0.8508, valloss=1.7597, Tavg=0.61101\n",
      "  12250/  15000: loss=0.8531, valloss=1.7629, Tavg=0.61111\n",
      "  12260/  15000: loss=0.8530, valloss=1.7593, Tavg=0.61121\n",
      "  12270/  15000: loss=0.8357, valloss=1.7307, Tavg=0.61131\n",
      "  12280/  15000: loss=0.8562, valloss=1.7293, Tavg=0.61140\n",
      "  12290/  15000: loss=0.8557, valloss=1.7588, Tavg=0.61151\n",
      "  12300/  15000: loss=0.8296, valloss=1.7725, Tavg=0.61160\n",
      "  12310/  15000: loss=0.8528, valloss=1.7509, Tavg=0.61171\n",
      "  12320/  15000: loss=0.8280, valloss=1.7849, Tavg=0.61180\n",
      "  12330/  15000: loss=0.8315, valloss=1.7358, Tavg=0.61190\n",
      "  12340/  15000: loss=0.8463, valloss=1.7403, Tavg=0.61200\n",
      "  12350/  15000: loss=0.8403, valloss=1.7445, Tavg=0.61210\n",
      "  12360/  15000: loss=0.8430, valloss=1.7499, Tavg=0.61220\n",
      "  12370/  15000: loss=0.8389, valloss=1.7612, Tavg=0.61230\n",
      "  12380/  15000: loss=0.8413, valloss=1.7433, Tavg=0.61240\n",
      "  12390/  15000: loss=0.8338, valloss=1.7596, Tavg=0.61250\n",
      "  12400/  15000: loss=0.8509, valloss=1.7528, Tavg=0.61260\n",
      "  12410/  15000: loss=0.8469, valloss=1.7633, Tavg=0.61270\n",
      "  12420/  15000: loss=0.8355, valloss=1.7510, Tavg=0.61281\n",
      "  12430/  15000: loss=0.8313, valloss=1.7677, Tavg=0.61291\n",
      "  12440/  15000: loss=0.8293, valloss=1.7348, Tavg=0.61301\n",
      "  12450/  15000: loss=0.8297, valloss=1.7559, Tavg=0.61311\n",
      "  12460/  15000: loss=0.8662, valloss=1.7749, Tavg=0.61321\n",
      "  12470/  15000: loss=0.8463, valloss=1.7667, Tavg=0.61331\n",
      "  12480/  15000: loss=0.8390, valloss=1.7556, Tavg=0.61341\n",
      "  12490/  15000: loss=0.8421, valloss=1.7916, Tavg=0.61351\n",
      "  12500/  15000: loss=0.8429, valloss=1.7714, Tavg=0.61361\n",
      "  12510/  15000: loss=0.8362, valloss=1.7671, Tavg=0.61371\n",
      "  12520/  15000: loss=0.8481, valloss=1.7850, Tavg=0.61381\n",
      "  12530/  15000: loss=0.8522, valloss=1.7629, Tavg=0.61391\n",
      "  12540/  15000: loss=0.8418, valloss=1.7417, Tavg=0.61400\n",
      "  12550/  15000: loss=0.8209, valloss=1.7714, Tavg=0.61410\n",
      "  12560/  15000: loss=0.8331, valloss=1.7616, Tavg=0.61420\n",
      "  12570/  15000: loss=0.8453, valloss=1.7704, Tavg=0.61430\n",
      "  12580/  15000: loss=0.8576, valloss=1.7770, Tavg=0.61440\n",
      "  12590/  15000: loss=0.8340, valloss=1.7914, Tavg=0.61450\n",
      "  12600/  15000: loss=0.8337, valloss=1.7905, Tavg=0.61460\n",
      "  12610/  15000: loss=0.8362, valloss=1.7734, Tavg=0.61470\n",
      "  12620/  15000: loss=0.8352, valloss=1.7520, Tavg=0.61479\n",
      "  12630/  15000: loss=0.8461, valloss=1.7667, Tavg=0.61488\n",
      "  12640/  15000: loss=0.8187, valloss=1.7708, Tavg=0.61498\n",
      "  12650/  15000: loss=0.8239, valloss=1.7897, Tavg=0.61507\n",
      "  12660/  15000: loss=0.8216, valloss=1.7797, Tavg=0.61517\n",
      "  12670/  15000: loss=0.8453, valloss=1.7552, Tavg=0.61527\n",
      "  12680/  15000: loss=0.8233, valloss=1.7920, Tavg=0.61537\n",
      "  12690/  15000: loss=0.8559, valloss=1.7847, Tavg=0.61546\n",
      "  12700/  15000: loss=0.8314, valloss=1.7810, Tavg=0.61556\n",
      "  12710/  15000: loss=0.8046, valloss=1.8166, Tavg=0.61565\n",
      "  12720/  15000: loss=0.8217, valloss=1.7864, Tavg=0.61574\n",
      "  12730/  15000: loss=0.8297, valloss=1.7596, Tavg=0.61584\n",
      "  12740/  15000: loss=0.8424, valloss=1.7761, Tavg=0.61593\n",
      "  12750/  15000: loss=0.8307, valloss=1.7813, Tavg=0.61602\n",
      "  12760/  15000: loss=0.8224, valloss=1.7769, Tavg=0.61612\n",
      "  12770/  15000: loss=0.8326, valloss=1.7687, Tavg=0.61621\n",
      "  12780/  15000: loss=0.8486, valloss=1.7818, Tavg=0.61631\n",
      "  12790/  15000: loss=0.8141, valloss=1.7557, Tavg=0.61640\n",
      "  12800/  15000: loss=0.8102, valloss=1.8052, Tavg=0.61649\n",
      "  12810/  15000: loss=0.8335, valloss=1.7948, Tavg=0.61658\n",
      "  12820/  15000: loss=0.8123, valloss=1.7941, Tavg=0.61667\n",
      "  12830/  15000: loss=0.8178, valloss=1.7922, Tavg=0.61676\n",
      "  12840/  15000: loss=0.8326, valloss=1.7978, Tavg=0.61686\n",
      "  12850/  15000: loss=0.8131, valloss=1.7933, Tavg=0.61695\n",
      "  12860/  15000: loss=0.8086, valloss=1.7708, Tavg=0.61705\n",
      "  12870/  15000: loss=0.8250, valloss=1.7826, Tavg=0.61714\n",
      "  12880/  15000: loss=0.7915, valloss=1.7783, Tavg=0.61724\n",
      "  12890/  15000: loss=0.8349, valloss=1.7839, Tavg=0.61732\n",
      "  12900/  15000: loss=0.8226, valloss=1.7867, Tavg=0.61741\n",
      "  12910/  15000: loss=0.8278, valloss=1.7804, Tavg=0.61750\n",
      "  12920/  15000: loss=0.8347, valloss=1.7938, Tavg=0.61759\n",
      "  12930/  15000: loss=0.8090, valloss=1.8096, Tavg=0.61769\n",
      "  12940/  15000: loss=0.8065, valloss=1.7707, Tavg=0.61778\n",
      "  12950/  15000: loss=0.8203, valloss=1.7994, Tavg=0.61787\n",
      "  12960/  15000: loss=0.8210, valloss=1.8194, Tavg=0.61796\n",
      "  12970/  15000: loss=0.8231, valloss=1.7850, Tavg=0.61805\n",
      "  12980/  15000: loss=0.8171, valloss=1.7861, Tavg=0.61814\n",
      "  12990/  15000: loss=0.8349, valloss=1.7812, Tavg=0.61823\n",
      "  13000/  15000: loss=0.8141, valloss=1.7958, Tavg=0.61832\n",
      "  13010/  15000: loss=0.8150, valloss=1.7882, Tavg=0.61841\n",
      "  13020/  15000: loss=0.8398, valloss=1.7921, Tavg=0.61850\n",
      "  13030/  15000: loss=0.8229, valloss=1.7847, Tavg=0.61859\n",
      "  13040/  15000: loss=0.8231, valloss=1.7974, Tavg=0.61868\n",
      "  13050/  15000: loss=0.8219, valloss=1.8061, Tavg=0.61877\n",
      "  13060/  15000: loss=0.8201, valloss=1.8001, Tavg=0.61886\n",
      "  13070/  15000: loss=0.8222, valloss=1.8016, Tavg=0.61896\n",
      "  13080/  15000: loss=0.7974, valloss=1.7968, Tavg=0.61905\n",
      "  13090/  15000: loss=0.8009, valloss=1.7961, Tavg=0.61913\n",
      "  13100/  15000: loss=0.8068, valloss=1.7840, Tavg=0.61922\n",
      "  13110/  15000: loss=0.8181, valloss=1.7997, Tavg=0.61931\n",
      "  13120/  15000: loss=0.8307, valloss=1.8014, Tavg=0.61940\n",
      "  13130/  15000: loss=0.8121, valloss=1.8345, Tavg=0.61949\n",
      "  13140/  15000: loss=0.8299, valloss=1.8023, Tavg=0.61958\n",
      "  13150/  15000: loss=0.8350, valloss=1.8104, Tavg=0.61967\n",
      "  13160/  15000: loss=0.7974, valloss=1.7981, Tavg=0.61976\n",
      "  13170/  15000: loss=0.8241, valloss=1.8025, Tavg=0.61984\n",
      "  13180/  15000: loss=0.8209, valloss=1.8085, Tavg=0.61993\n",
      "  13190/  15000: loss=0.8245, valloss=1.8025, Tavg=0.62002\n",
      "  13200/  15000: loss=0.8050, valloss=1.7910, Tavg=0.62011\n",
      "  13210/  15000: loss=0.8073, valloss=1.8121, Tavg=0.62020\n",
      "  13220/  15000: loss=0.8041, valloss=1.8070, Tavg=0.62029\n",
      "  13230/  15000: loss=0.8181, valloss=1.8065, Tavg=0.62038\n",
      "  13240/  15000: loss=0.8153, valloss=1.8025, Tavg=0.62047\n",
      "  13250/  15000: loss=0.7917, valloss=1.8168, Tavg=0.62056\n",
      "  13260/  15000: loss=0.8022, valloss=1.8228, Tavg=0.62064\n",
      "  13270/  15000: loss=0.8248, valloss=1.8206, Tavg=0.62073\n",
      "  13280/  15000: loss=0.8133, valloss=1.8048, Tavg=0.62081\n",
      "  13290/  15000: loss=0.8153, valloss=1.8152, Tavg=0.62090\n",
      "  13300/  15000: loss=0.8285, valloss=1.8132, Tavg=0.62099\n",
      "  13310/  15000: loss=0.7952, valloss=1.8197, Tavg=0.62107\n",
      "  13320/  15000: loss=0.8081, valloss=1.7988, Tavg=0.62116\n",
      "  13330/  15000: loss=0.8200, valloss=1.8271, Tavg=0.62124\n",
      "  13340/  15000: loss=0.8148, valloss=1.8020, Tavg=0.62133\n",
      "  13350/  15000: loss=0.8047, valloss=1.8180, Tavg=0.62142\n",
      "  13360/  15000: loss=0.8054, valloss=1.8042, Tavg=0.62151\n",
      "  13370/  15000: loss=0.7945, valloss=1.8046, Tavg=0.62159\n",
      "  13380/  15000: loss=0.8019, valloss=1.8108, Tavg=0.62167\n",
      "  13390/  15000: loss=0.7983, valloss=1.8035, Tavg=0.62176\n",
      "  13400/  15000: loss=0.8115, valloss=1.8079, Tavg=0.62184\n",
      "  13410/  15000: loss=0.7849, valloss=1.8326, Tavg=0.62192\n",
      "  13420/  15000: loss=0.8207, valloss=1.8246, Tavg=0.62201\n",
      "  13430/  15000: loss=0.8104, valloss=1.8326, Tavg=0.62210\n",
      "  13440/  15000: loss=0.8155, valloss=1.8532, Tavg=0.62218\n",
      "  13450/  15000: loss=0.8120, valloss=1.8284, Tavg=0.62227\n",
      "  13460/  15000: loss=0.7936, valloss=1.8357, Tavg=0.62235\n",
      "  13470/  15000: loss=0.8046, valloss=1.8142, Tavg=0.62244\n",
      "  13480/  15000: loss=0.8158, valloss=1.8250, Tavg=0.62252\n",
      "  13490/  15000: loss=0.8141, valloss=1.8207, Tavg=0.62261\n",
      "  13500/  15000: loss=0.7882, valloss=1.7968, Tavg=0.62269\n",
      "  13510/  15000: loss=0.8034, valloss=1.8370, Tavg=0.62278\n",
      "  13520/  15000: loss=0.8141, valloss=1.8020, Tavg=0.62287\n",
      "  13530/  15000: loss=0.7914, valloss=1.8326, Tavg=0.62295\n",
      "  13540/  15000: loss=0.7925, valloss=1.8225, Tavg=0.62303\n",
      "  13550/  15000: loss=0.7998, valloss=1.8255, Tavg=0.62312\n",
      "  13560/  15000: loss=0.7949, valloss=1.8282, Tavg=0.62320\n",
      "  13570/  15000: loss=0.7903, valloss=1.8440, Tavg=0.62328\n",
      "  13580/  15000: loss=0.8070, valloss=1.8221, Tavg=0.62336\n",
      "  13590/  15000: loss=0.8056, valloss=1.8098, Tavg=0.62345\n",
      "  13600/  15000: loss=0.7746, valloss=1.7998, Tavg=0.62353\n",
      "  13610/  15000: loss=0.7913, valloss=1.8405, Tavg=0.62362\n",
      "  13620/  15000: loss=0.7843, valloss=1.8321, Tavg=0.62370\n",
      "  13630/  15000: loss=0.7805, valloss=1.8285, Tavg=0.62378\n",
      "  13640/  15000: loss=0.7993, valloss=1.8505, Tavg=0.62386\n",
      "  13650/  15000: loss=0.8145, valloss=1.8417, Tavg=0.62394\n",
      "  13660/  15000: loss=0.8106, valloss=1.8351, Tavg=0.62403\n",
      "  13670/  15000: loss=0.7806, valloss=1.8186, Tavg=0.62410\n",
      "  13680/  15000: loss=0.7971, valloss=1.8492, Tavg=0.62419\n",
      "  13690/  15000: loss=0.8151, valloss=1.8346, Tavg=0.62427\n",
      "  13700/  15000: loss=0.8001, valloss=1.8530, Tavg=0.62435\n",
      "  13710/  15000: loss=0.7671, valloss=1.8066, Tavg=0.62443\n",
      "  13720/  15000: loss=0.8026, valloss=1.8188, Tavg=0.62451\n",
      "  13730/  15000: loss=0.7927, valloss=1.8208, Tavg=0.62459\n",
      "  13740/  15000: loss=0.8141, valloss=1.8421, Tavg=0.62467\n",
      "  13750/  15000: loss=0.7976, valloss=1.8208, Tavg=0.62475\n",
      "  13760/  15000: loss=0.8152, valloss=1.8212, Tavg=0.62483\n",
      "  13770/  15000: loss=0.7962, valloss=1.8071, Tavg=0.62491\n",
      "  13780/  15000: loss=0.7716, valloss=1.8372, Tavg=0.62499\n",
      "  13790/  15000: loss=0.7810, valloss=1.8434, Tavg=0.62507\n",
      "  13800/  15000: loss=0.8033, valloss=1.8355, Tavg=0.62515\n",
      "  13810/  15000: loss=0.7965, valloss=1.8402, Tavg=0.62523\n",
      "  13820/  15000: loss=0.7851, valloss=1.8439, Tavg=0.62531\n",
      "  13830/  15000: loss=0.8011, valloss=1.8319, Tavg=0.62539\n",
      "  13840/  15000: loss=0.7948, valloss=1.8412, Tavg=0.62547\n",
      "  13850/  15000: loss=0.7817, valloss=1.8467, Tavg=0.62554\n",
      "  13860/  15000: loss=0.8221, valloss=1.8343, Tavg=0.62562\n",
      "  13870/  15000: loss=0.7850, valloss=1.8504, Tavg=0.62570\n",
      "  13880/  15000: loss=0.7888, valloss=1.8226, Tavg=0.62578\n",
      "  13890/  15000: loss=0.8005, valloss=1.8325, Tavg=0.62586\n",
      "  13900/  15000: loss=0.7989, valloss=1.8503, Tavg=0.62594\n",
      "  13910/  15000: loss=0.7882, valloss=1.8387, Tavg=0.62602\n",
      "  13920/  15000: loss=0.7827, valloss=1.8571, Tavg=0.62610\n",
      "  13930/  15000: loss=0.7587, valloss=1.8329, Tavg=0.62618\n",
      "  13940/  15000: loss=0.7821, valloss=1.8593, Tavg=0.62625\n",
      "  13950/  15000: loss=0.7726, valloss=1.8501, Tavg=0.62633\n",
      "  13960/  15000: loss=0.7811, valloss=1.8652, Tavg=0.62641\n",
      "  13970/  15000: loss=0.7648, valloss=1.8637, Tavg=0.62649\n",
      "  13980/  15000: loss=0.7674, valloss=1.8539, Tavg=0.62657\n",
      "  13990/  15000: loss=0.7905, valloss=1.8418, Tavg=0.62665\n",
      "  14000/  15000: loss=0.7861, valloss=1.8630, Tavg=0.62673\n",
      "  14010/  15000: loss=0.7875, valloss=1.8766, Tavg=0.62681\n",
      "  14020/  15000: loss=0.7897, valloss=1.8544, Tavg=0.62688\n",
      "  14030/  15000: loss=0.7790, valloss=1.8607, Tavg=0.62696\n",
      "  14040/  15000: loss=0.7844, valloss=1.8520, Tavg=0.62703\n",
      "  14050/  15000: loss=0.8025, valloss=1.8473, Tavg=0.62711\n",
      "  14060/  15000: loss=0.7819, valloss=1.8506, Tavg=0.62718\n",
      "  14070/  15000: loss=0.7889, valloss=1.8508, Tavg=0.62726\n",
      "  14080/  15000: loss=0.7960, valloss=1.8579, Tavg=0.62734\n",
      "  14090/  15000: loss=0.7884, valloss=1.8573, Tavg=0.62741\n",
      "  14100/  15000: loss=0.7802, valloss=1.8287, Tavg=0.62749\n",
      "  14110/  15000: loss=0.7718, valloss=1.8507, Tavg=0.62757\n",
      "  14120/  15000: loss=0.7904, valloss=1.8707, Tavg=0.62765\n",
      "  14130/  15000: loss=0.7828, valloss=1.8467, Tavg=0.62772\n",
      "  14140/  15000: loss=0.7550, valloss=1.8675, Tavg=0.62779\n",
      "  14150/  15000: loss=0.7800, valloss=1.8737, Tavg=0.62786\n",
      "  14160/  15000: loss=0.7946, valloss=1.8583, Tavg=0.62794\n",
      "  14170/  15000: loss=0.7845, valloss=1.8565, Tavg=0.62802\n",
      "  14180/  15000: loss=0.7818, valloss=1.8594, Tavg=0.62809\n",
      "  14190/  15000: loss=0.7727, valloss=1.8575, Tavg=0.62817\n",
      "  14200/  15000: loss=0.7538, valloss=1.8391, Tavg=0.62824\n",
      "  14210/  15000: loss=0.7851, valloss=1.8527, Tavg=0.62832\n",
      "  14220/  15000: loss=0.7597, valloss=1.8625, Tavg=0.62839\n",
      "  14230/  15000: loss=0.7610, valloss=1.8528, Tavg=0.62847\n",
      "  14240/  15000: loss=0.7982, valloss=1.8805, Tavg=0.62854\n",
      "  14250/  15000: loss=0.7851, valloss=1.8595, Tavg=0.62861\n",
      "  14260/  15000: loss=0.7640, valloss=1.8473, Tavg=0.62869\n",
      "  14270/  15000: loss=0.7746, valloss=1.8699, Tavg=0.62876\n",
      "  14280/  15000: loss=0.7700, valloss=1.8567, Tavg=0.62884\n",
      "  14290/  15000: loss=0.7676, valloss=1.8650, Tavg=0.62891\n",
      "  14300/  15000: loss=0.7578, valloss=1.8562, Tavg=0.62898\n",
      "  14310/  15000: loss=0.7803, valloss=1.8729, Tavg=0.62906\n",
      "  14320/  15000: loss=0.7689, valloss=1.8513, Tavg=0.62913\n",
      "  14330/  15000: loss=0.7890, valloss=1.8834, Tavg=0.62920\n",
      "  14340/  15000: loss=0.7860, valloss=1.8553, Tavg=0.62928\n",
      "  14350/  15000: loss=0.7643, valloss=1.8562, Tavg=0.62935\n",
      "  14360/  15000: loss=0.7791, valloss=1.8473, Tavg=0.62942\n",
      "  14370/  15000: loss=0.7857, valloss=1.8753, Tavg=0.62950\n",
      "  14380/  15000: loss=0.7464, valloss=1.8701, Tavg=0.62957\n",
      "  14390/  15000: loss=0.7843, valloss=1.8627, Tavg=0.62965\n",
      "  14400/  15000: loss=0.7543, valloss=1.8636, Tavg=0.62972\n",
      "  14410/  15000: loss=0.8026, valloss=1.8694, Tavg=0.62979\n",
      "  14420/  15000: loss=0.7791, valloss=1.8570, Tavg=0.62987\n",
      "  14430/  15000: loss=0.7577, valloss=1.8709, Tavg=0.62994\n",
      "  14440/  15000: loss=0.7945, valloss=1.8688, Tavg=0.63001\n",
      "  14450/  15000: loss=0.7596, valloss=1.8676, Tavg=0.63009\n",
      "  14460/  15000: loss=0.7660, valloss=1.8507, Tavg=0.63016\n",
      "  14470/  15000: loss=0.7721, valloss=1.8707, Tavg=0.63023\n",
      "  14480/  15000: loss=0.7633, valloss=1.8488, Tavg=0.63031\n",
      "  14490/  15000: loss=0.7531, valloss=1.8868, Tavg=0.63038\n",
      "  14500/  15000: loss=0.7474, valloss=1.8779, Tavg=0.63045\n",
      "  14510/  15000: loss=0.7699, valloss=1.8804, Tavg=0.63053\n",
      "  14520/  15000: loss=0.7635, valloss=1.8844, Tavg=0.63060\n",
      "  14530/  15000: loss=0.7732, valloss=1.8731, Tavg=0.63068\n",
      "  14540/  15000: loss=0.7554, valloss=1.8520, Tavg=0.63075\n",
      "  14550/  15000: loss=0.7617, valloss=1.8598, Tavg=0.63082\n",
      "  14560/  15000: loss=0.7661, valloss=1.8823, Tavg=0.63090\n",
      "  14570/  15000: loss=0.7425, valloss=1.8802, Tavg=0.63097\n",
      "  14580/  15000: loss=0.7652, valloss=1.8662, Tavg=0.63104\n",
      "  14590/  15000: loss=0.7776, valloss=1.8858, Tavg=0.63111\n",
      "  14600/  15000: loss=0.7566, valloss=1.8682, Tavg=0.63118\n",
      "  14610/  15000: loss=0.7796, valloss=1.8541, Tavg=0.63125\n",
      "  14620/  15000: loss=0.7508, valloss=1.8813, Tavg=0.63132\n",
      "  14630/  15000: loss=0.7706, valloss=1.8809, Tavg=0.63139\n",
      "  14640/  15000: loss=0.7658, valloss=1.8751, Tavg=0.63146\n",
      "  14650/  15000: loss=0.7440, valloss=1.8941, Tavg=0.63153\n",
      "  14660/  15000: loss=0.7606, valloss=1.8673, Tavg=0.63161\n",
      "  14670/  15000: loss=0.7827, valloss=1.8918, Tavg=0.63168\n",
      "  14680/  15000: loss=0.7721, valloss=1.8812, Tavg=0.63175\n",
      "  14690/  15000: loss=0.7392, valloss=1.8696, Tavg=0.63182\n",
      "  14700/  15000: loss=0.7444, valloss=1.9051, Tavg=0.63189\n",
      "  14710/  15000: loss=0.7596, valloss=1.8719, Tavg=0.63195\n",
      "  14720/  15000: loss=0.7528, valloss=1.8849, Tavg=0.63203\n",
      "  14730/  15000: loss=0.7642, valloss=1.8903, Tavg=0.63210\n",
      "  14740/  15000: loss=0.7609, valloss=1.8903, Tavg=0.63217\n",
      "  14750/  15000: loss=0.7681, valloss=1.8959, Tavg=0.63224\n",
      "  14760/  15000: loss=0.7400, valloss=1.8846, Tavg=0.63232\n",
      "  14770/  15000: loss=0.7825, valloss=1.8680, Tavg=0.63239\n",
      "  14780/  15000: loss=0.7517, valloss=1.9007, Tavg=0.63246\n",
      "  14790/  15000: loss=0.7486, valloss=1.9015, Tavg=0.63253\n",
      "  14800/  15000: loss=0.7343, valloss=1.8928, Tavg=0.63260\n",
      "  14810/  15000: loss=0.7592, valloss=1.8804, Tavg=0.63266\n",
      "  14820/  15000: loss=0.7606, valloss=1.8776, Tavg=0.63273\n",
      "  14830/  15000: loss=0.7378, valloss=1.9209, Tavg=0.63280\n",
      "  14840/  15000: loss=0.7265, valloss=1.8723, Tavg=0.63287\n",
      "  14850/  15000: loss=0.7138, valloss=1.8980, Tavg=0.63293\n",
      "  14860/  15000: loss=0.7747, valloss=1.8775, Tavg=0.63300\n",
      "  14870/  15000: loss=0.7410, valloss=1.9090, Tavg=0.63308\n",
      "  14880/  15000: loss=0.7548, valloss=1.9118, Tavg=0.63315\n",
      "  14890/  15000: loss=0.7515, valloss=1.8969, Tavg=0.63321\n",
      "  14900/  15000: loss=0.7450, valloss=1.9432, Tavg=0.63328\n",
      "  14910/  15000: loss=0.7586, valloss=1.8629, Tavg=0.63334\n",
      "  14920/  15000: loss=0.7630, valloss=1.8806, Tavg=0.63341\n",
      "  14930/  15000: loss=0.7588, valloss=1.8688, Tavg=0.63347\n",
      "  14940/  15000: loss=0.7472, valloss=1.8939, Tavg=0.63354\n",
      "  14950/  15000: loss=0.7614, valloss=1.8949, Tavg=0.63361\n",
      "  14960/  15000: loss=0.7487, valloss=1.9070, Tavg=0.63368\n",
      "  14970/  15000: loss=0.7292, valloss=1.9079, Tavg=0.63374\n",
      "  14980/  15000: loss=0.7456, valloss=1.8974, Tavg=0.63381\n",
      "  14990/  15000: loss=0.7701, valloss=1.8890, Tavg=0.63388\n",
      "  15000/  15000: loss=0.7420, valloss=1.9163, Tavg=0.63395\n",
      "  15000/  15000: loss=0.7420, Tavg=0.63395\n"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(9999)\n",
    "batch_size = 64\n",
    "max_s = 15_000\n",
    "DTbar = 0.0\n",
    "valloss = float('inf')\n",
    "print(f\"{steps:7d}/{max_s:7d}: loss={loss.item():1.4f}, valloss={estimate_loss(m, 20)[\"val\"]:.4f}, Tavg={DTbar/1e9:.5f}\")\n",
    "while steps < max_s:\n",
    "    steps += 1\n",
    "    Ta = time_ns()\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    DT = time_ns() - Ta\n",
    "    DTbar = (DTbar*steps + DT)/(steps+1)\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(f\"{steps:7d}/{max_s:7d}: loss={loss.item():1.4f}, valloss={estimate_loss(m, 20)[\"val\"]:.4f}, Tavg={DTbar/1e9:.5f}\")\n",
    "    \n",
    "print(f\"{max_s:7d}/{max_s:7d}: loss={loss.item():1.4f}, Tavg={DTbar/1e9:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e288145-4fee-4b72-8303-56a6112ae927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(0.4973), 'val': tensor(1.9065)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss(m, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f7feefc-2e50-40f5-aecc-e489b78cc03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6cfffbfd-863f-480f-9a10-66cb202712e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = torch.zeros((1, 1), dtype = torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4473413e-60aa-4cf1-9278-06a64f891b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span>Tell my most greeting arm our request\n",
       "Is passing correction than the day of alliants,\n",
       "Brought the Englishmanen Hump of Henrage to\n",
       "open the Lordshire of Rome, whose wrongs purpose,\n",
       "And gave comfort! Whom w then? I have could I had on\n",
       "Shall on me wrong, too rouse to the Tower.\n",
       "And she must live to Angelo, as I have,\n",
       "And leave unsafely to thine, I beseech you,\n",
       "Honesty nature for your beings to yourself;\n",
       "For now I am you with, but that dispatch:\n",
       "Mend Yirkshire came my very stumper&#39;d:\n",
       "And now &#39;twould say &#39;I that give us heart\n",
       "And long be castor&#39;d Henry&#39;s goodly queen,\n",
       "For that she stirs hath been an achieve&#39;s eyes,\n",
       "Along heavy burree in jest.\n",
       "\n",
       "KING RICHARD II:\n",
       "All purchased, that still is she laid,\n",
       "His golty mind the Phoetra conspirates\n",
       "Was never to command, or on armorant foe,\n",
       "One kingdon&#39;d up by chanliness, not took damn&#39;d\n",
       "Deperseth end.\n",
       "\n",
       "BRUTUS:\n",
       "Be thus death&#39;d i&#39; the matter:\n",
       "Life, away!\n",
       "\n",
       "Constable, nurse,nes, coward!\n",
       "\n",
       "CAMILLO:\n",
       "\n",
       "MENENIUS:\n",
       "Hence, a brothelm!\n",
       "Why, you are comfortable. What is this?\n",
       "\n",
       "VOLUMNIA:\n",
       "Not for my best good fast, not yourself, I crept,\n",
       "That in the yur consuls wonder of Calais.\n",
       "\n",
       "CLUDIO:\n",
       "The best knows you to cunning t\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "Tell my most greeting arm our request\n",
       "Is passing correction than the day of alliants,\n",
       "Brought the Englishmanen Hump of Henrage to\n",
       "open the Lordshire of Rome, whose wrongs purpose,\n",
       "And gave comfort! Whom w then? I have could I had on\n",
       "Shall on me wrong, too rouse to the Tower.\n",
       "And she must live to Angelo, as I have,\n",
       "And leave unsafely to thine, I beseech you,\n",
       "Honesty nature for your beings to yourself;\n",
       "For now I am you with, but that dispatch:\n",
       "Mend Yirkshire came my very stumper\\PYZsq{}d:\n",
       "And now \\PYZsq{}twould say \\PYZsq{}I that give us heart\n",
       "And long be castor\\PYZsq{}d Henry\\PYZsq{}s goodly queen,\n",
       "For that she stirs hath been an achieve\\PYZsq{}s eyes,\n",
       "Along heavy burree in jest.\n",
       "\n",
       "KING RICHARD II:\n",
       "All purchased, that still is she laid,\n",
       "His golty mind the Phoetra conspirates\n",
       "Was never to command, or on armorant foe,\n",
       "One kingdon\\PYZsq{}d up by chanliness, not took damn\\PYZsq{}d\n",
       "Deperseth end.\n",
       "\n",
       "BRUTUS:\n",
       "Be thus death\\PYZsq{}d i\\PYZsq{} the matter:\n",
       "Life, away!\n",
       "\n",
       "Constable, nurse,nes, coward!\n",
       "\n",
       "CAMILLO:\n",
       "\n",
       "MENENIUS:\n",
       "Hence, a brothelm!\n",
       "Why, you are comfortable. What is this?\n",
       "\n",
       "VOLUMNIA:\n",
       "Not for my best good fast, not yourself, I crept,\n",
       "That in the yur consuls wonder of Calais.\n",
       "\n",
       "CLUDIO:\n",
       "The best knows you to cunning t\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "Tell my most greeting arm our request\n",
       "Is passing correction than the day of alliants,\n",
       "Brought the Englishmanen Hump of Henrage to\n",
       "open the Lordshire of Rome, whose wrongs purpose,\n",
       "And gave comfort! Whom w then? I have could I had on\n",
       "Shall on me wrong, too rouse to the Tower.\n",
       "And she must live to Angelo, as I have,\n",
       "And leave unsafely to thine, I beseech you,\n",
       "Honesty nature for your beings to yourself;\n",
       "For now I am you with, but that dispatch:\n",
       "Mend Yirkshire came my very stumper'd:\n",
       "And now 'twould say 'I that give us heart\n",
       "And long be castor'd Henry's goodly queen,\n",
       "For that she stirs hath been an achieve's eyes,\n",
       "Along heavy burree in jest.\n",
       "\n",
       "KING RICHARD II:\n",
       "All purchased, that still is she laid,\n",
       "His golty mind the Phoetra conspirates\n",
       "Was never to command, or on armorant foe,\n",
       "One kingdon'd up by chanliness, not took damn'd\n",
       "Deperseth end.\n",
       "\n",
       "BRUTUS:\n",
       "Be thus death'd i' the matter:\n",
       "Life, away!\n",
       "\n",
       "Constable, nurse,nes, coward!\n",
       "\n",
       "CAMILLO:\n",
       "\n",
       "MENENIUS:\n",
       "Hence, a brothelm!\n",
       "Why, you are comfortable. What is this?\n",
       "\n",
       "VOLUMNIA:\n",
       "Not for my best good fast, not yourself, I crept,\n",
       "That in the yur consuls wonder of Calais.\n",
       "\n",
       "CLUDIO:\n",
       "The best knows you to cunning t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Code, clear_output\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     display(Code(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mdecode(prompt[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())))\n",
      "Cell \u001b[0;32mIn[23], line 55\u001b[0m, in \u001b[0;36mBigramLanguageModel.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# crop to the last #block_size tokens\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     idx_cond \u001b[38;5;241m=\u001b[39m idx[:, \u001b[38;5;241m-\u001b[39mblock_size:]\n\u001b[0;32m---> 55\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# look at the last time step to predict what comes next\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m# (B, C)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# invoke self-attention\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# layernorm before logits\u001b[39;00m\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# residual connections by adding self \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Code, clear_output\n",
    "\n",
    "while True:\n",
    "    prompt = m.generate(prompt, max_new_tokens=1)\n",
    "    clear_output(wait=True)\n",
    "    display(Code(language='text', data=decode(prompt[0].tolist())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
