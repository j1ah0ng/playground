{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7c7097c5-45b2-4156-9f1e-1e3fb33088cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b94c70d8-1375-4331-9722-86dcb2501860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed words into a vector space\n",
    "# initialize them randomly\n",
    "# words move around in a vector space. goal is to align semantically similar words\n",
    "#  in the vector space.\n",
    "# embeddings let you infer locally, on out-of-distribution input during inference.\n",
    "#  => generalize\n",
    "# how do you train an encoder?\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a67852c7-8b9e-4277-937b-8125e85aa6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map characters\n",
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a8419e52-0e10-4c33-bcf8-3e78333d41fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "\n",
    "context_length = 3\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:]:\n",
    "#    print(w)\n",
    "    context = [0] * context_length # padded context\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch] # the integer index of current ch\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # \"crop\", and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "69ef178f-1ace-4fde-a097-7cf00f510ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# 27 possible characters. (our tokens are characters.)\n",
    "# cram them into 2d.\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100)) # catted first layers --> 100 middle layers\n",
    "b1 = torch.randn(100) # bias\n",
    "# create the output / final layer.\n",
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2dfd0a5f-2d34-4628-9686-02e0102bff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1210861206054688\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "\n",
    "    # minibatching.\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    emb = C[X[ix]] # indexing into C to get the embeddings\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100). 32 is the size of the dset. 100 is the intermediate layer.\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "   # print(loss.item())\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = 10**-4\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "#    lri.append(lrexp[i])\n",
    "#    lossi.append(loss.item())\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "aa4d7b06-e4a7-4483-a086-e8fbd11fe850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.314883232116699"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6)@W1+b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
